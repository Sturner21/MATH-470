---
title: "Analysis2"
format: html
---

# Load Libraries

```{r}
#| warning: false
library(tidyverse)
library(rstan)
library(bayesplot)
library(Lahman)
```

```{r}
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# Clean Lahman

```{r}
#| message: false

#Creates a DF called "Batting" which has players stats from 1973-2019

#Load the DF's from Lahman directly
Batting <- Lahman::Batting
Teams <- Lahman::Teams
People <- Lahman::People

Batting <- Batting %>% 
  #Minimum year = DH was introduced; maximum year = before COVID
  filter(yearID >= 1973 & yearID <= 2019)

#Filter teams for years considered and select only necessary columns
Teams <- Teams %>% 
  filter(yearID>=1973 & yearID <= 2019) %>% 
  select(yearID, teamID, park)

#Combine current "Batting" and "Teams" DF to associate player with stadium played at
Batting <- Batting %>% 
  right_join(Teams, by = join_by(yearID, teamID))

#Add player's age to the "Batting" DF
Batting <- Batting %>% 
  right_join(Lahman::People, by = join_by(playerID)) %>% 
  mutate(Age = yearID - birthYear, .after=yearID) %>%
  #Don't consider the people who don't have a birthday in the database
  drop_na(Age)

#Make the lgID variable into an index variable
Batting <- Batting %>% 
  mutate(lg = case_when(lgID == "NL" ~ 1,
                        lgID == "AL" ~ 2), .after=lgID)
```

```{r}
#We need to make the data here "slim" like is suggested by McElreath
slim <- list(
  N = nrow(Batting),
  z = n_distinct(Batting %>% pull(yearID)),
  Year = as.integer(Batting %>% pull(yearID) - 1972),
  Age = as.integer(Batting %>% pull(Age)-17),
  LG = as.integer(Batting %>% pull(lg)),
  AB = Batting %>% pull(AB),
  HR = Batting %>% pull(HR),
  SB = Batting %>% pull(SB),
  BirthYear = as.integer(Batting %>% pull(birthYear)-1922)
)
```

```{r}
#Define logit for convenience

inv_logit <- function(p){
  1 / (1 + exp(-p))
}
```

```{r}
#Create the summarized data frame

Sum.Batting <- Batting %>% 
  group_by(yearID, Age) %>% 
  summarize(AB = sum(AB),
            HR = sum(HR))
```

```{r}
#Make an index for the parks to add them to the model

#The index goes from 1-88. No idea how it came up with them but I kept the park name in Batting so anyone should be able to reference to see how the parks impact player performance

Batting <- Batting %>% 
  mutate(park = as.factor(park)) %>% 
  mutate(park_index = as.integer(park), .after = park)
```

```{r}
#Create a column which described the decade a player was born in

Batting %>% 
  #Put the birth year down to the decade value. (i.e. 1936 becomes 193.6)
  #Then force it to be an integer. R doesn't round so we are left with (1936=193) a decade
  mutate(birthDecade = as.integer(birthYear/10), .before=birthYear) %>% 
  #Then, index it by making the minimum year equal to 1
  mutate(birthDecade = birthDecade - min(birthDecade) + 1)
```

# Theoretical Model

The model used by Fellingham & Fisher (2017) is structured as,

$$
HR_{i,j}|p_{i,j}\sim Binom(AB_{i,j},p_{i,j})\\
logit(p_{i,j})=x_{i,j}\beta_i+\theta_i+\delta_j+\xi_p
$$

Where $\theta_i$ is the effect of decade of birth, $\delta_j$ is the effect of season of play, and $\xi_p$ is the effect of home ballpark.

However, I think that this model could be improved and should be structured as,

$$
HR_{i,j}\sim Binom(AB_{i,j},p_{i,j})\\
logit(p_{i,j})=\beta_1Age+\beta_2Ballpark+\beta_3Year+\beta_4SB+\beta_5(Age\cdot Year)
$$

All of these variables should influence the underlying probability of a player to hit a home run. In addition, I think that Age and Year played will have some sort of interaction effect. Possibly not during the small time frame that we consider here, however over a longer study I believe it should.

# Graphical exploration

Before building a model, we explore the data by graphing the home-run proportion with classical binomial standard errors.

```{r}
Batting_summary <- Batting %>% group_by(Age) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>%
  mutate(prop = HR / AB, se = sqrt(prop*(1-prop)/AB))

ggplot(data = Batting_summary) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))
```

Let's graph again, restricting to players aged 20 to 40.

```{r}
ggplot(data = Batting_summary %>% filter(Age >= 20 & Age <= 40)) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))
```

Our first model will include a linear term for age and will be restricted to this age range.

# Simple model

```{r}
Batting_summary_restricted <- Batting_summary %>%
  filter(Age >= 20 & Age <= 40) %>% mutate(c.Age = Age - 30)

simple_data <-   list(
  N = nrow(Batting_summary_restricted),
  Age = Batting_summary_restricted %>% pull(c.Age),
  AB = Batting_summary_restricted %>% pull(AB),
  HR = Batting_summary_restricted %>% pull(HR)
)
```

We start with a frequentist fit using maximum likelihood.

```{r}
glm(cbind(HR, AB - HR) ~ c.Age, data = Batting_summary_restricted, family = binomial)
```

And now let's fit using the following simple model:

```         
data:
N = number of ages (21 between 20 and 40)
AB = at-bats, pooled for each age
HR = home runs, pooled for each age
Age = age, recentered at 30 for easier interpretation of intercept

parameters:
beta0 (intercept for logistic model)
beta1 (slope for logistic model)

model:

For n in 1:N,

HR[n] ~ binomial(AB[n], inv_logit(beta0 + beta1 * Age[n]))
```

```{r}
fit_simple <- stan(
  file = "HR2_simple.stan",
  data = simple_data
)
```

The fitted parameters match the frequentist maximum likelihood parameters. (But there now no mystery about the meaning of the standard errors.)

```{r}
summary(fit_simple, pars = c("beta0", "beta1"))$summary %>% signif(3)
```

We can make a plot of the parameters, but there is not much to see, since we can estimate the parameters quite precisely thanks to the huge data set.

```{r}
plot(fit_simple, pars = c("beta0", "beta1"))
```

We can get a sense for the fitted model by sampling from the posterior. We take 10 random rows and plot the model lines with the empirical logits of the proportions. Yuck! This model does not fit the data well at all. Room for improvement! (Quadratic model... Splines...)

```{r}
logit <- function(p) log( p / (1 - p))

post_simple <- as.data.frame(fit_simple) %>% select(beta0, beta1) %>%
  slice_sample(n = 10)

ggplot(data = Batting_summary_restricted) +
  geom_point(mapping = aes(x = c.Age, y = logit(prop))) +
  geom_abline(slope = post_simple$beta1, intercept = post_simple$beta0,
              alpha = 0.3)
```

# Empirical Model

```{r}
fit1 <- stan(
  file = "HR21.stan",
  data = slim,
  chains = 4,
  warmup = 1000,
  iter = 1000,
  cores = 4,
  refresh = 1000
)
```

```{r}
plot(fit1, pars = "a")
```

```{r}
plot(fit1, pars = "b")
```

```{r}
fit2 <- stan(
  file = "HR22.stan",
  data = slim,
  chains = 1,
  warmup = 1000,
  iter = 4000,
  cores = 1,
  refresh = 1000
)
```

```{r}
plot(fit2, pars = "a")
```

```{r}
plot(fit2, pars = "b")
```

```{r}
plot(fit2, pars = "c")
```

```{r}
data_23 <- list(
  N = nrow(Batting),
  n_year = n_distinct(Batting$yearID),
  AB = Batting %>% pull(AB),
  HR = Batting %>% pull(HR),
  Year = Batting %>% pull(yearID) - 1972,
  Age = Batting %>% pull(Age) - 30
)
```

```{r}
fit_23 <- stan(
  file = "HR23.stan",
  data = data_23
)
```

```{r}
summary(fit_23)$summary %>% signif(3)
```

```{r}
plot(fit_23, pars = "b")
```

```{r}
saveRDS(fit_23, "HR23_new.rds")
```

```{r}
traceplot(fit_23)
```

```{r}
sum_2001 <- Batting %>% filter(yearID == 2001) %>% group_by(Age) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>% mutate(prop = HR / AB)
```

```{r}
post_sample <- as.data.frame(fit_23, pars = c("b[29]", "beta1")) %>% slice_sample(n = 10) 

ggplot(data = sum_2001) + 
  geom_point(mapping = aes(x = Age - 30, y = logit(prop))) +
  geom_abline(slope = post_sample$beta1, intercept = post_sample$"b[29]")
```

Try a new version of the model:

```{r}
sum_all_years <- Batting %>% group_by(yearID, Age) %>%
  summarize(HR = sum(HR), AB = sum(AB), SB = sum(SB))
```

Fit a model first with varying intercepts (different intercept for each year) but constant slope.

Start with just 200 iterations to see if the model works at all.

```{r}
data23b <- list(
  N = nrow(sum_all_years),
  n_year = n_distinct(sum_all_years$yearID),
  AB = sum_all_years %>% pull(AB),
  HR = sum_all_years %>% pull(HR),
  Year = sum_all_years %>% pull(yearID) -1972,
  Age = sum_all_years %>% pull(Age) -30
)
```

```{r}
#I find that 500 works and 200 doesn't
fit23b <- stan(
  file = "HR23b.stan",
  data = data23b,
  iter = 500
)
```

```{r}
summary(fit23b)$summary %>% signif(3)
```

We should try to see if we can implement player archetype into the model. We can use SB to represent players archetype. Basically, we have the big slugger who doesn't steal bags and the skinny fast player who does steal bags.

```{r}
no_age_sum_all_years <- sum_all_years %>% 
  group_by(yearID) %>% 
  summarize(HR = sum(HR),
            AB = sum(AB),
            SB = sum(SB)) %>% 
  mutate(p = HR / AB)
```

```{r}
#Lets check graphically

ggplot(data=no_age_sum_all_years)+
  aes(x=SB, y=HR)+
  geom_point()
```

When ignoring age, we can see a general downward trend for HR proportion and age over the time series we are considering.

```{r}
ggplot(data = no_age_sum_all_years)+
  aes(x=yearID, y=p)+
  geom_line()
```

```{r}
ggplot(data=no_age_sum_all_years)+
  aes(x=yearID, y=SB)+
  geom_line()
```

This confirms my hypothesis that players are starting to hit home runs with more frequency at the trade off of stolen bases. Those who remain stealing bags are may not be hitting home runs and thus we should at least try adding SB's into the model to see how it does.

```{r}
data24 <- list(
  N = nrow(sum_all_years),
  n_year = n_distinct(sum_all_years$yearID),
  AB = sum_all_years %>% pull(AB),
  HR = sum_all_years %>% pull(HR),
  Year = sum_all_years %>% pull(yearID) -1972,
  Age = sum_all_years %>% pull(Age) -30,
  SB = sum_all_years %>% pull(SB)
)
```

```{r}
fit24 <- stan(
  file = "HR24.stan",
  data = data24,
  iter = 2000
)
```

Iterating using both 500 and 1000 loops leaves an error. Suggesting that using SB probably is not a good idea. Or at least we should talk about it more before implementation.

```{r}
summary(fit24)$summary
```

```{r}
plot(fit24, pars = c("beta1", "c"))
```

### Another try with SB

Let's try a new version of the model above that separates players by SB as well as age. Before we do anything, we can look at home-run proportions when we group player seasons (cases in our data set) by the number of stolen bases. We first graph the proportions.

```{r}
Batting_summary_SB <- Batting %>% group_by(SB) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>%
  mutate(prop = HR / AB, se = sqrt(prop*(1-prop)/AB))

ggplot(data = Batting_summary_SB) +
  geom_pointrange(mapping = aes(x = SB, y = prop, ymin = prop - se,
                                ymax = prop + se))
```

We can graph again on the logit scale, focusing on cases where the number of stolen bases is less than 50. The association looks plausibly linear, suggesting that a logit-binomial model may be reasonable.

```{r}
logit <- function(p){log(p / (1 - p))}

ggplot(data = Batting_summary_SB %>% filter(SB < 50)) +
  geom_pointrange(mapping = aes(x = SB, y = logit(prop), ymin = logit(prop - se),
                                ymax = logit(prop + se))) +
  geom_smooth(method = "lm", mapping = aes(x = SB, y = logit(prop)))
```

Since we will use both stolen bases and age in the model, we can make the same sort of graph after grouping players by age. Let's look first at the distribution of age.

```{r}
ggplot(data = Batting) +
  geom_histogram(mapping = aes(x = Age), binwidth = 1)
```

(What a lovely histogram!)

We will split players into three roughly equally sized age groups, determined by quantiles.

```{r}
Batting %>% summarize(quantile(Age, 0.33), quantile(Age, 0.67))
```

Our three groups are "26 or younger", "27 to 30", and "over 30".

```{r}
Batting <- Batting %>% mutate(age_group = case_when(
  Age <= 26 ~ "young",
  Age <= 30 ~ "middle",
  Age > 30 ~ "old"
))
```

We repeat our graphical exploration, faceting by age group. The general pattern (negative association between SB and home-run probability) persists across age groups. We note, though, that the pattern looks a bit different for the youngest players

```{r}
Batting_summary_age_SB <- Batting %>% group_by(age_group, SB) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>%
  mutate(prop = HR / AB, se = sqrt(prop*(1-prop)/AB))

ggplot(data = Batting_summary_age_SB) +
  geom_pointrange(mapping = aes(x = SB, y = prop, ymin = prop - se,
                                ymax = prop + se)) +
  facet_wrap(~age_group)
```

```{r}
ggplot(data = Batting_summary_age_SB) +
  geom_pointrange(mapping = aes(x = SB, y = logit(prop), ymin = prop - se,
                                ymax = prop + se)) +
  facet_wrap(~age_group)
```

To prepare to fit the model, we summarize the data, grouping player-seasons by year, player age, and number of stolen bases. (As in other examples, this grouping will not affect the likelihood, and it seems to speed up the work of Stan considerably.)

```{r}
sum_all_years_SB <- Batting %>% group_by(yearID, Age, SB) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>% mutate(prop = HR / AB) %>% ungroup
```

```{r}
data24b <- list(
  N = nrow(sum_all_years_SB),
  n_year = n_distinct(sum_all_years_SB$yearID),
  AB = sum_all_years_SB %>% pull(AB),
  HR = sum_all_years_SB %>% pull(HR),
  Year = sum_all_years_SB %>% pull(yearID) -1972,
  Age = sum_all_years_SB %>% pull(Age) -30,
  SB = sum_all_years_SB %>% pull(SB)
)
```

Before we fit the model, we can simulate data from the prior distributions. We just generate one simulated data set, since there is plenty of internal replication in the data set.

```{r}
prior24b <- stan(
  file = "HR24b_prior_sim.stan",
  data = data24b,
  algorithm = "Fixed_param",
  iter = 1,
  chains = 1
)
```

To get an overall sense for the simulated data, we look at simulated home-run proportions in the groups of players (by year, age, and stolen bases).

```{r}
HR_sim <- (rstan :: extract)(prior24b)$HR[1,]

sum_all_years_SB <- sum_all_years_SB %>% mutate(HR_sim = HR_sim,
                                                prop_sim = HR_sim / AB)
```

```{r}
ggplot(data = sum_all_years_SB) +
  geom_histogram(mapping = aes(x = prop_sim))
```

Here is the actual distribution of HR proportions in the data. We shouldn't be trying to fit the data with the prior, but we can see how far off things are from this second histogram.

```{r}
ggplot(data = sum_all_years_SB) +
  geom_histogram(mapping = aes(x = prop))
```

Huh! The Bayesians warn us about this phenomenon. (See Bayesian Workflow, Section 2.4 and Figure 3.) Weak independent priors on large numbers of parameters add up to something strong. The home-run proportions are either near 0% or 100%. If we did not have a lot of data to fit the model, we would have to use stronger priors to get something reasonable here. In our case, there is so much data that the prior distributions do not matter much (at least for this model).

We can repeat setting stronger priors.

```{r}
prior24b2 <- stan(
  file = "HR24b_prior_sim2.stan",
  data = data24b,
  algorithm = "Fixed_param",
  iter = 1,
  chains = 1
)
```

```{r}
HR_sim2 <- (rstan :: extract)(prior24b2)$HR[1,]

sum_all_years_SB2 <- sum_all_years_SB %>% mutate(HR_sim = HR_sim2,
                                                prop_sim = HR_sim / AB)
```

```{r}
ggplot(data = sum_all_years_SB2) +
  geom_histogram(mapping = aes(x = prop_sim))
```

That prior distribution of simulated HR proportions looks better than what we got with the very weak priors on `beta1` and `c`. But perhaps these priors would be too strong...

The next chunk fits the model.

```{r}
fit24b <- stan(
  file = "HR24b.stan",
  data = data24b,
  iter = 500
)
```

We have another version of the model that also generates replications.

(Here is an RStan demo with some inspirational code for working with the replications: \[http://avehtari.github.io/BDA_R_demos/demos_rstan/ppc/poisson-ppc.html\]. Here is another vignette with examples of posterior predictive checks: \[https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html\])

A better way to do this would be to remove the rows in the data table for playes with 0 at-bats. We could then have Stan calculate proportions (without divide-by-0 errors). I got around those rows in an ugly way below.

```{r}
fit24b_reps <- stan(
  file = "HR24b_reps.stan",
  data = data24b,
  iter = 500
)
```

```{r}
HR_rep <- as.matrix(fit24b_reps, pars = "HR_rep")

dim(HR_rep)
```

```{r}
ppc_hist(sum_all_years_SB %>% filter(AB > 0) %>% pull(prop) ,
         HR_rep[1:8, sum_all_years_SB$AB > 0] / (sum_all_years_SB %>% filter(AB > 0) %>% pull(AB))) +
  xlim(0, 0.3)
```

Yuck... A poor fit. The model replications are quite consistent---and don't look much like the original data. In the model, home-run percentages over 10% and as high as 20% sometimes occur. These never happen in the real data. The distribution of proportions in the simulated data has mode at close to 0. In the real data, the model appears closer to 3%.

```{r}
ppc_dens_overlay(sum_all_years_SB %>% filter(AB > 0) %>% pull(prop) ,
         HR_rep[1:50, sum_all_years_SB$AB > 0] / (sum_all_years_SB %>% filter(AB > 0) %>% pull(AB))) +
  xlim(0, 0.3)
```

This family of density plots shows the same thing as the histograms. The data from the model are quite consistent---and quite different from the original data.

Perhaps I will add more here, trying some better posterior plots from the bayesplot package. See, for example, this page \[https://mc-stan.org/bayesplot/reference/PPC-errors.html\], which shows how to make the kind of binned-residual graphs for binomial models that we used in MATH 313.

## Multilevel Model

Maybe we can start with the model we used in "HR23.stan,"

$$
HR\sim Binom(AB,p)\\
p=b(Year)+\beta_1Age\\
b\sim Normal(-3.5,0.1)\\
\beta_1\sim Normal(0,1)
$$

And change it to something like

$$
HR\sim Binom(AB,p)\\
p=b(Year)+\beta_1Age\\
b\sim Normal(\bar{b},\sigma_b)\\
\bar{b}\sim Normal(-3.5,0.1)\\
\sigma_b\sim Exp(1)\\
\beta_1\sim Normal(0,1)
$$

This mirrors the differences in the models used on pages 402-403.

```{r}
#We don't need to import any more data than is already present in HR23 so we can use that dataset again.

#After running the first time (which didn't take long. Yay!) it suggested that I should run some more iterations so I'll up it to 1000

fit25 <- stan(
  file = "HR25.stan",
  data = data23b,
  iter = 1000
)
```

Doing 1000 iterations removed any problems I had with the model, and I believe that I was able to accurately do the (simple) multilevel model.

```{r}
summary(fit25)$summary %>% signif(3)
```

```{r}
plot(fit25, pars = "b")
```

```{r}
traceplot(fit25)
```

```{r}
saveRDS(fit25, "HR25.rds")
```

Not sure that I did this right so I'm going to leave it here for now.

I also wanted to investigate how park factors can play into a hitter's ability. So, let's add them into the model here. As a note, we don't need to create a diferent coefficient for every year since the ballpark doesn't move.

```{r}
sum_all_years_park <- Batting %>% group_by(yearID, park, park_index, Age) %>%
  summarize(HR = sum(HR), AB = sum(AB), SB = sum(SB))
```

```{r}
data26 <- list(
  N = nrow(sum_all_years_park),
  n_year = n_distinct(sum_all_years_park$yearID),
  AB = sum_all_years_park %>% pull(AB),
  HR = sum_all_years_park %>% pull(HR),
  Year = sum_all_years_park %>% pull(yearID) -1972,
  Age = sum_all_years_park %>% pull(Age) -30,
  park = sum_all_years_park %>% pull(park_index)
)
```

```{r}
fit26 <- stan(
  file = "HR26.stan",
  data = data26,
  iter = 500
)
```

Even 500 iterations did not work and took 15 minutes so I think it's safe to say that adding the stadium might not be the greatest idea until we get more computing power. However, I still want to save this.

```{r}
saveRDS(fit26, "HR26.rds")
```

Of course, the error that is being reported is one of poor priors, so I'll try and go in and give it some more confident priors.

```{r}
fit26b <- stan(
  file = "HR26b.stan",
  data = data26,
  iter = 400
)
```

```{r}
saveRDS(fit26b, "HR26b.rds")
```

Still get the warning after 400 iterations AND making the priors more informative. Not sure why this is behaving so poorly, there is such good theory to support this hypothesis. Of course, this (along with the problems Dr. Parson was facing) could explain why Fellingham and Fisher had to do over 200,000 iterations (I believe it was that crazy amount).

## Graphical Exploration pt.2

Do we see evidence of probability changing with age and year?

```{r}
p_year_age <- Batting %>% 
  group_by(yearID, Age) %>% 
  summarize(HR = sum(HR), AB = sum(AB)) %>% 
  mutate(p = HR / AB)
```

```{r}
ggplot(data = p_year_age)+
  aes(x=yearID, y=p, color=as.factor(Age))+
  geom_line()
```

Clearly, we aren't getting anything from this graph. So, let's graph two more, one with the player's age rounded to the it's 10 (ie. 32-\> 30 and 36 -\> 30) and one where we use the cut-offs from the prior examination (below 27, 27 to 30, and above 30).

```{r}
p_year_age <- p_year_age %>% 
  mutate(age_10 = Age / 10, .after = Age) %>% 
  mutate(age_10 = as.integer(age_10)) %>% 
  mutate(age_group = case_when(Age < 27 ~ "young",
                               Age >= 27 & Age <= 30 ~ "medial",
                               Age > 30 ~ "old") , .after = age_10) %>% 
  mutate(age_group = as.factor(age_group)) %>% 
  mutate(age_10 = as.factor(age_10))
```

```{r}
ggplot(data = p_year_age)+
  aes(x = yearID, y = p, color = age_10)+
  geom_line()+
  geom_point()
```

```{r}
ggplot(data = p_year_age)+
  aes(x = yearID, y = p, color = age_group)+
  geom_line()+
  geom_point()
```

Based on these graphs, there does not seem to be good evidence to justify fitting a different slope for each age in each year. They all tend to the same amounts. Other than the 50+ players and the 18 and 19 year-olds. However, I suspect that this is due much more to a lack of a representitive sample than anything else.

Another things we wanted to check was the fit of a polynomial with degree 4 to see what Fellingham and Fisher were getting at with their Detrich process. We already have a graph but it's at the top so let's bring it down to see it again.

```{r}
Batting_summary2 <- Batting_summary %>% 
  filter(Age >=20 & Age <= 40)
```

```{r}
ggplot(data = Batting_summary2) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))
```

We can run an lm to get a general sense of what these coefficients should be

```{r}
quart <- lm(data = Batting_summary2, prop ~ Age + I(Age^2) + I(Age^3) + I(Age^4)) 

quart %>% 
  summary()
```

Not that it means much but we do get significance on each coefficient. I find this interesting because I've never seen a degree 4 polynomial have such significance.

```{r}
ggplot(data = Batting_summary2) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))+
  geom_function(fun = function(x) (2.364e-01) + (-3.334e-02)*x + (1.871e-03)*(x^2) + (-4.463e-05)*(x^3) + (3.873e-07)*(x^4), color="red")
```

So, very clearly this has a much better fit to the age variable. How does it look for the full data set we considered (not just the 20-40 year-olds).

```{r}
ggplot(data = Batting_summary) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))+
  geom_function(fun = function(x) (2.364e-01) + (-3.334e-02)*x + (1.871e-03)*(x^2) + (-4.463e-05)*(x^3) + (3.873e-07)*(x^4), color="red")
```

This is actually pretty funny. We lose all the fit past the age of 40 when considering the whole data set. Since we can't even justify the 4th degree and this doesn't fit past 40 it probably isn't a good idea to use this fit. Of course, if all we care about is fit, then we could just keep addign degrees until all years considered are accounted for.
