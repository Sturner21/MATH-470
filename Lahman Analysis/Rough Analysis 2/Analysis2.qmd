---
title: "Analysis2"
format: html
---

# Load Libraries

```{r}
#| warning: false
library(tidyverse)
library(rstan)
library(Lahman)
```

```{r}
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

# Clean Lahman

```{r}
#| message: false

#Creates a DF called "Batting" which has players stats from 1973-2019

#Load the DF's from Lahman directly
Batting <- Lahman::Batting
Teams <- Lahman::Teams
People <- Lahman::People

Batting <- Batting %>% 
  #Minimum year = DH was introduced; maximum year = before COVID
  filter(yearID >= 1973 & yearID <= 2019)

#Filter teams for years considered and select only necessary columns
Teams <- Teams %>% 
  filter(yearID>=1973 & yearID <= 2019) %>% 
  select(yearID, teamID, park)

#Combine current "Batting" and "Teams" DF to associate player with stadium played at
Batting <- Batting %>% 
  right_join(Teams, by = join_by(yearID, teamID))

#Add player's age to the "Batting" DF
Batting <- Batting %>% 
  right_join(Lahman::People, by = join_by(playerID)) %>% 
  mutate(Age = yearID - birthYear, .after=yearID) %>%
  #Don't consider the people who don't have a birthday in the database
  drop_na(Age)

#Make the lgID variable into an index variable
Batting <- Batting %>% 
  mutate(lg = case_when(lgID == "NL" ~ 1,
                        lgID == "AL" ~ 2), .after=lgID)
```

```{r}
#We need to make the data here "slim" like is suggested by McElreath
slim <- list(
  N = nrow(Batting),
  z = n_distinct(Batting %>% pull(yearID)),
  Year = as.integer(Batting %>% pull(yearID) - 1972),
  Age = as.integer(Batting %>% pull(Age)-17),
  LG = as.integer(Batting %>% pull(lg)),
  AB = Batting %>% pull(AB),
  HR = Batting %>% pull(HR),
  SB = Batting %>% pull(SB),
  BirthYear = as.integer(Batting %>% pull(birthYear)-1922)
)
```

```{r}
#Define logit for convenience

inv_logit <- function(p){
  1 / (1 + exp(-p))
}
```

```{r}
#Create the summarized data frame

Sum.Batting <- Batting %>% 
  group_by(yearID, Age) %>% 
  summarize(AB = sum(AB),
            HR = sum(HR))
```

# Theoretical Model

The model used by Fellingham & Fisher (2017) is structured as,

$$
HR_{i,j}|p_{i,j}\sim Binom(AB_{i,j},p_{i,j})\\
logit(p_{i,j})=x_{i,j}\beta_i+\theta_i+\delta_j+\xi_p
$$

Where $\theta_i$ is the effect of decade of birth, $\delta_j$ is the effect of season of play, and $\xi_p$ is the effect of home ballpark.

However, I think that this model could be improved and should be structured as,

$$
HR_{i,j}\sim Binom(AB_{i,j},p_{i,j})\\
logit(p_{i,j})=\beta_1Age+\beta_2Ballpark+\beta_3Year+\beta_4SB+\beta_5(Age\cdot Year)
$$

All of these variables should influence the underlying probability of a player to hit a home run. In addition, I think that Age and Year played will have some sort of interaction effect. Possibly not during the small time frame that we consider here, however over a longer study I believe it should.

# Graphical exploration

Before building a model, we explore the data by graphing the home-run proportion with classical binomial standard errors.

```{r}
Batting_summary <- Batting %>% group_by(Age) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>%
  mutate(prop = HR / AB, se = sqrt(prop*(1-prop)/AB))

ggplot(data = Batting_summary) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))
```

Let's graph again, restricting to players aged 20 to 40.

```{r}
ggplot(data = Batting_summary %>% filter(Age >= 20 & Age <= 40)) +
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - se,
                                ymax = prop + se))
```

Our first model will include a linear term for age and will be restricted to this age range.

# Simple model

```{r}
Batting_summary_restricted <- Batting_summary %>%
  filter(Age >= 20 & Age <= 40) %>% mutate(c.Age = Age - 30)

simple_data <-   list(
  N = nrow(Batting_summary_restricted),
  Age = Batting_summary_restricted %>% pull(c.Age),
  AB = Batting_summary_restricted %>% pull(AB),
  HR = Batting_summary_restricted %>% pull(HR)
)
```

We start with a frequentist fit using maximum likelihood.

```{r}
glm(cbind(HR, AB - HR) ~ c.Age, data = Batting_summary_restricted, family = binomial)
```

And now let's fit using the following simple model:

```         
data:
N = number of ages (21 between 20 and 40)
AB = at-bats, pooled for each age
HR = home runs, pooled for each age
Age = age, recentered at 30 for easier interpretation of intercept

parameters:
beta0 (intercept for logistic model)
beta1 (slope for logistic model)

model:

For n in 1:N,

HR[n] ~ binomial(AB[n], inv_logit(beta0 + beta1 * Age[n]))
```

```{r}
fit_simple <- stan(
  file = "HR2_simple.stan",
  data = simple_data
)
```

The fitted parameters match the frequentist maximum likelihood parameters. (But there now no mystery about the meaning of the standard errors.)

```{r}
summary(fit_simple, pars = c("beta0", "beta1"))$summary %>% signif(3)
```

We can make a plot of the parameters, but there is not much to see, since we can estimate the parameters quite precisely thanks to the huge data set.

```{r}
plot(fit_simple, pars = c("beta0", "beta1"))
```

We can get a sense for the fitted model by sampling from the posterior. We take 10 random rows and plot the model lines with the empirical logits of the proportions. Yuck! This model does not fit the data well at all. Room for improvement! (Quadratic model... Splines...)

```{r}
logit <- function(p) log( p / (1 - p))

post_simple <- as.data.frame(fit_simple) %>% select(beta0, beta1) %>%
  slice_sample(n = 10)

ggplot(data = Batting_summary_restricted) +
  geom_point(mapping = aes(x = c.Age, y = logit(prop))) +
  geom_abline(slope = post_simple$beta1, intercept = post_simple$beta0,
              alpha = 0.3)
```

# Empirical Model

```{r}
fit1 <- stan(
  file = "HR21.stan",
  data = slim,
  chains = 4,
  warmup = 1000,
  iter = 1000,
  cores = 4,
  refresh = 1000
)
```

```{r}
plot(fit1, pars = "a")
```

```{r}
plot(fit1, pars = "b")
```

```{r}
fit2 <- stan(
  file = "HR22.stan",
  data = slim,
  chains = 1,
  warmup = 1000,
  iter = 4000,
  cores = 1,
  refresh = 1000
)
```

```{r}
plot(fit2, pars = "a")
```

```{r}
plot(fit2, pars = "b")
```

```{r}
plot(fit2, pars = "c")
```

```{r}
data_23 <- list(
  N = nrow(Batting),
  n_year = n_distinct(Batting$yearID),
  AB = Batting %>% pull(AB),
  HR = Batting %>% pull(HR),
  Year = Batting %>% pull(yearID) - 1972,
  Age = Batting %>% pull(Age) - 30
)
```

```{r}
fit_23 <- stan(
  file = "HR23.stan",
  data = data_23
)
```

```{r}
summary(fit_23)$summary %>% signif(3)
```

```{r}
plot(fit_23, pars = "b")
```

```{r}
saveRDS(fit_23, "HR23_new.rds")
```

```{r}
traceplot(fit_23)
```

```{r}
sum_2001 <- Batting %>% filter(yearID == 2001) %>% group_by(Age) %>%
  summarize(HR = sum(HR), AB = sum(AB)) %>% mutate(prop = HR / AB)
```

```{r}
post_sample <- as.data.frame(fit_23, pars = c("b[29]", "beta1")) %>% slice_sample(n = 10) 

ggplot(data = sum_2001) + 
  geom_point(mapping = aes(x = Age - 30, y = logit(prop))) +
  geom_abline(slope = post_sample$beta1, intercept = post_sample$"b[29]")
```

Try a new version of the model:

```{r}
sum_all_years <- Batting %>% group_by(yearID, Age) %>%
  summarize(HR = sum(HR), AB = sum(AB), SB = sum(SB))
```

Fit a model first with varying intercepts (different intercept for each year) but constant slope.

Start with just 200 iterations to see if the model works at all.

```{r}
data23b <- list(
  N = nrow(sum_all_years),
  n_year = n_distinct(sum_all_years$yearID),
  AB = sum_all_years %>% pull(AB),
  HR = sum_all_years %>% pull(HR),
  Year = sum_all_years %>% pull(yearID) -1972,
  Age = sum_all_years %>% pull(Age) -30
)
```

```{r}
#I find that 500 works and 200 doesn't
fit23b <- stan(
  file = "HR23b.stan",
  data = data23b,
  iter = 500
)
```

```{r}
summary(fit23b)$summary %>% signif(3)
```

We should try to see if we can implement player archetype into the model. We can use SB to represent players archetype. Basically, we have the big slugger who doesn't steal bags and the skinny fast player who does steal bags.

```{r}
no_age_sum_all_years <- sum_all_years %>% 
  group_by(yearID) %>% 
  summarize(HR = sum(HR),
            AB = sum(AB),
            SB = sum(SB)) %>% 
  mutate(p = HR / AB)
```

```{r}
#Lets check graphically

ggplot(data=no_age_sum_all_years)+
  aes(x=SB, y=HR)+
  geom_point()
```

When ignoring age, we can see a general downward trend for HR proportion and age over the time series we are considering.

```{r}
ggplot(data = no_age_sum_all_years)+
  aes(x=yearID, y=p)+
  geom_line()
```

```{r}
ggplot(data=no_age_sum_all_years)+
  aes(x=yearID, y=SB)+
  geom_line()
```

This confirms my hypothesis that players are starting to hit home runs with more frequency at the trade off of stolen bases. Those who remain stealing bags are may not be hitting home runs and thus we should at least try adding SB's into the model to see how it does.

```{r}
data24 <- list(
  N = nrow(sum_all_years),
  n_year = n_distinct(sum_all_years$yearID),
  AB = sum_all_years %>% pull(AB),
  HR = sum_all_years %>% pull(HR),
  Year = sum_all_years %>% pull(yearID) -1972,
  Age = sum_all_years %>% pull(Age) -30,
  SB = sum_all_years %>% pull(SB)
)
```

```{r}
fit23b <- stan(
  file = "HR24.stan",
  data = data24,
  iter = 500
)
```

Iterating using both 500 and 1000 loops leaves an error. Suggesting that using SB probably is not a good idea. Or at least we should talk about it more before implementation.

## Multilevel Model

Maybe we can start with the model we used in "HR23.stan,"

$$
HR\sim Binom(AB,p)\\
p=b(Year)+\beta_1Age\\
b\sim Normal(-3.5,0.1)\\
\beta_1\sim Normal(0,1)
$$

And change it to something like

$$
HR\sim Binom(AB,p)\\
p=b(Year)+\beta_1Age\\
b\sim Normal(\bar{b},\sigma_b)\\
\bar{b}\sim Normal(-3.5,0.1)\\
\sigma_b\sim Exp(1)\\
\beta_1\sim Normal(0,1)
$$

This mirrors the differences in the models used on pages 402-403.

```{r}
#We don't need to import any more data than is already present in HR23 so we can use that dataset again.

#After running the first time (which didn't take long. Yay!) it suggested that I should run some more iterations so I'll up it to 1000

fit25 <- stan(
  file = "HR25.stan",
  data = data23b,
  iter = 1000
)
```

Doing 1000 iterations removed any problems I had with the model, and I believe that I was able to accurately do the (simple) multilevel model.

```{r}
summary(fit25)$summary %>% signif(3)
```

```{r}
plot(fit25, pars = "b")
```

```{r}
traceplot(fit25)
```

```{r}
saveRDS(fit25, "HR25.rds")
```

Not sure that I did this right so I'm going to leave it here for now.
