---
title: "Statistical Grand Slams"
subtitle: "A Bayesian Approach to Modeling Home Run Production in Major League Baseball"
author: "Sam Turner"
date: today
format: 
  html:
    toc: true
    toc-location: left
    toc-expand: true
    embed-resources: true
    number-sections: true
    
execute: 
  echo: false
  warning: false
---

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(scales)
library(rstan)
library(Lahman)
library(bayesplot)
library(tidybayes)
library(corrplot)
library(boot)
```

```{r}
#Set Stan to read the amount of cores that your computer can handel
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
#| message: false

#Creates a DF called "Batting" which has players stats from 1973-2019

#Load the DF's from Lahman directly
Batting <- Lahman::Batting
Teams <- Lahman::Teams
People <- Lahman::People

Batting <- Batting %>% 
  #Minimum year = DH was introduced; maximum year = before COVID
  filter(yearID >= 1973 & yearID <= 2019)

#Filter teams for years considered and select only necessary columns
Teams <- Teams %>% 
  filter(yearID>=1973 & yearID <= 2019) %>% 
  select(yearID, teamID, park)

#Combine current "Batting" and "Teams" DF to associate player with stadium played at
Batting <- Batting %>% 
  right_join(Teams, by = join_by(yearID, teamID))

#Add player's age to the "Batting" DF
Batting <- Batting %>% 
  right_join(Lahman::People, by = join_by(playerID)) %>% 
  mutate(Age = yearID - birthYear, .after=yearID) %>%
  #Don't consider the people who don't have a birthday in the database
  drop_na(Age)

#Make the lgID variable into an index variable
Batting <- Batting %>% 
  mutate(lg = case_when(lgID == "NL" ~ 1,
                        lgID == "AL" ~ 2), .after=lgID)


#Make an index for the parks to add them to the model

#The index goes from 1-88. No idea how it came up with them but I kept the park name in Batting so anyone should be able to reference to see how the parks impact player performance

Batting <- Batting %>% 
  mutate(park = as.factor(park)) %>% 
  mutate(park_index = as.integer(park), .after = park)

#Find qualified players who have over 50 AB's in each of the minimum 6 seasons played from 1973-2019
qualified_players <- Batting %>% group_by(playerID, Age) %>% 
  summarize(AB = sum(AB),
            HR = sum(HR)) %>% 
  filter(min(AB) >= 50, n() >= 6)

#Filter Batting DF for qualified players
Batting <- Batting %>% 
  filter(playerID %in% qualified_players$playerID)

#Calculated each players HR proportion "p"
Batting <- Batting %>% 
  mutate(p = HR / AB, .after=AB)
#You can see this for yourself too, but there are 657 players in this dataframe now

#Create an index for each player
Batting <- Batting %>% 
  mutate(player_index = as.factor(playerID), .after = playerID) %>% 
  mutate(player_index = as.integer(player_index))

#Create a centered age variable by subtracting 30 from everyone's age and square it for non-liner effect

Batting <- Batting %>% 
  mutate(cAge = Age -30, .after = Age) %>% 
  mutate(cAgeSq = cAge**2, .after = cAge)

#Make a year index because it's easier than having to create one everytime

Batting <- Batting %>% 
  mutate(year_index = yearID -1972, .after = yearID)
```

```{r}
#Make the data needed for the correlation plot
Batting_cor <- Batting %>%
  mutate(PA = AB + BB + IBB + HBP + SH + SF) %>% 
  select(AB, HR, PA, p, park_index, yearID, cAge) %>%
  filter(!is.na(p))
```

```{r}
#| eval: false
#Choose 500 players at random to consider
#If this doesn't take too long, then we can come back later and change this to possibly consider all players. Or just widen the initial time-frame considered and do 1000 players

set.seed(500)

N_player <- Batting %>% 
  distinct(playerID) %>% 
  pull(playerID) %>% 
  sample(size=500, replace=FALSE)
```

```{r}
#| eval: false
#Make a dataframe that only keeps the 500 players
Samp_Batting <- Batting %>% 
  filter(playerID %in% N_player)

head(Samp_Batting)
#If curious, this seed and selection covers all years. So, it should be able to handle the year-to-year effects
```

# Introduction and Motivations {#sec-intro}

To apply the methods discussed in *Statistical Rethinking* by Richard McElreath, Dr. Parson and I decided to replicate *Predicting Home Run Production in Major League Baseball Using a Bayesian Semiparametric Model* by Gilbert Fellingham and Jared Fisher. This article in particular has drawn our attention because of some unique and interesting modeling choices made by the authors. Both Dr. Parson and I believe that the initial investigation by Fellingham and Fisher (2017) is interesting, but there are some questionable modeling choices made by the authors throughout the paper. For example, there are few predictor variables and their associated priors are very weak. The paper does a good job with displaying plenty of visualizations for the reader, yet, under careful examination one is left with the impression that the authors have chosen to display a non-random assortment of players. This leaves careful readers wondering if the validity of their model is constant across all players, or if they have chosen a certain subset because these special players justify the use of their model. These reasons in tandem bring Fellingham and Fisher's (2017) findings into question. Thus, Dr. Parson and I have embarked to investigate the same research question as them: can we use a multilevel Bayesian model to predict a player's home run performance?

If modeling for the sake of learning was not reason enough to embark on this endeavor, there is the possibility of financial compensation also looming with this research question. If one is able to accurately predict the HR's that a player hits in a season, then they would probably be able to gain employment for a MLB team. HR's are the pinnacle of a hitter's trip to the plate, they are guaranteed to score at least one run and can lead to up to four being scored depending on how many people are on base. Thus, MLB teams which seek to maximize wins are likely looking to hire analysts who can accurately predict the HR's a player would hit so that they know who to target for drafting, signing, and trading. In addition, sports betting has recently become legal in many states across the United States. Thus, if someone is able to accurately predict HR's before a season starts, they would be able to make a large amount of money from gambling. Dr. Parson and I are more interested in the former scenario concerning gainful employment, but other readers may not share our interests and it is fair to point out the obvious consequences of this research.

# Baseball Background {#sec-BaseBack}

In Major League Baseball (MLB) the team on offense is allowed to send one player, at a time, up to home plate to "bat." Any time a player is sent up to bat this constitutes a plate appearance (PA). Depending on the outcome of the PA it may or may not be considered an at-bat (AB). If the PA ends in the player being walked (BB \[Base on Balls\]) then it is not recorded as an AB because it is thought the player did not receive a fair chance at hitting the baseball. On the other hand, if the PA ends in the player getting a hit or getting out, then it is assumed that he had a fair chance at hitting the ball and the PA is recorded as an AB. This distinction to the average fan is mostly arbitrary. However, in a modeling context it is very important. When modeling a player's ability to hit home runs (HR), as this paper is concerned with, the distinction is important because it is unfair to consider all the PA's of a player since this set of outcomes contains outcomes where it is assumed that the player has not been given a fair chance to hit the ball. For similar reasons, considering the subset of outcomes where it is assumed that they player has been given a fair chance to hit the ball seems more appropriate, this subset being AB's. Thus, this paper uses AB's as the main predictor for home runs hit by a certain player. Fellingham and Fisher (2017) also agree with this reasoning and use AB's, rather than PA's, in their research.

Furthermore, it is logical to ask: why not consider both? Incorporating more predictors can often make a model more accurate at prediction; and both Fellingham and Fisher (2017) and this paper are concerned with maximizing predictive power of our respective models. The reason this paper does not consider both is because of multicollinearity. Multicollinearity is where two independent variables in a model are highly or perfectly correlated and the model is unable to determine which variable is responsible for the corresponding change in the depending variable. Without a doubt there is a strong correlation between PA's and AB's because one is a subset of the other. In fact, we can see that the correlation between the two is quite strong in @fig-BattingCor. If we included both PA's and AB's in our model, our model's algorithm Markov Chain Monte Carlo (MCMC) would have a difficult time determining which of the two variables is responsible for the changes in the dependent variable, HR's. For other generalized linear models (GLM's) it becomes impossible to deal with multicollinearity and either the model parameters need to be changed or the model is deemed impossible to calculate. Thus, we neglect PA's from our model both because theory stipulates it is an inferior predictor and used in tandem with our current predictor it would cause problems for the modeling software.

An interesting quirk of baseball is that, like soccer, stadiums differ in dimensions. This is not universal; baseball fields and stadiums must have the same measurements for their infields. Yet, for the most part baseball stadiums are allowed to configure their outfields as they please. This becomes important because most HR's are achieved by hitting the baseball over the outfield fence. Thus, fences closer to home plate generally lead to more HR's for all hitters, ceteris paribus. The non-standardized nature of stadiums' fences gives them a variety of unique shapes and ranging outfield sizes. For example, Fenway Park (1923-) is known to have the closest outfield wall with their "Green Monster" in left field being only 305 ft. from home plate. While the Polo Grounds (1891-1963) boasted one of the furthest walls from home plate at 483 ft. in center field. Stadiums also differ in location. Marlins Park (2012-2020) sits at 15 ft. above sea level while Coors Field (1995-) sits at 5183 ft. above sea level. These changes in elevation allow the ball to fly farther in the air, and hence also impact HR rates. As illustrated there are not only vastly different dimensions that baseball fields can take on but also different environments which affect a player's ability to hit home runs. Thus, any model which attempts to model players' ability to hit HR's needs to consider ballpark factors.

MLB has existed since about 1871. Since its inception the game has undergone some incredible transformations which have vastly changed the nature of the game, making it into what we know today. For example, from 1900-1920 baseball was in the "deadball era." During this period the materials used to create baseballs made it so that the ball absorbed much more force than it does today. This had a plethora of implications for the game and how it was played; for example, HR's were much less common then they are today because they were harder to hit. In contrast, the period from 1994-2004 is known informally as the "steroid era" because of the rampant use of performance enhancing drugs (PED) used by players. This, in combination with other factors such as harder materials used to make baseballs, led to players obliterating a variety of previous hitting records, such as the career and season home runs records. It is obvious from these facts that the decade played for player's had a drastic impact on their ability to produce some outcomes, such as HR's. In fact, this can be visualized by considering all the runs scored during a given season from 1871-2022 displayed in @fig-RunsByYear, or considering the total HR's hit from 1871-2022 displayed in @fig-HRByYear.

Finally, age is an important factor in athletic ability. In MLB, seniority can act both as both a blessing and a curse. On the one hand, older players are less able to hit home runs because their body is starting to lose its peak athletic performance compared to when it was, say, 30. On the other hand, these older players also have more experience than younger players and thus may be better able to hit home runs from strategies they have found during their time playing. Further we can visualize this by looking at the median HR's hit by each player from 1871-2022, which is displayed in @fig-HR.By.Age.All. However, this graph does not depict some of the nuances of the data well. For example, we have almost no data on how often 18 -year-old players hit HR's because they play in MLB infrequently. As can be seen with the standard error bars on the ages above 40, older players suffer a similar problem of small sample size. Instead, consider @fig-HR.By.Age.20to40 which only displays players who are 20-40 years old during the season. This graph clearly shows the complex non-linear relationship between player performance and age. It shows that players are generally improving in their ability to hit HR's from 20-30 years old. Then, however, we see a slight downwards trend followed by an upward trend with increasing error bars from ages 30-40. This shows that as players age past 30 their abilities start to decline. However, the jump afterwards can probably be attributed to players who are very good, continuing their careers. If a player is very good then they are going to be able to perform at above average when they are, say, 35. However, if the player is not generally considered good, then a team will likely opt to try out a new young rookie who may not be as good in the moment but is more projectable. Thus, we see less players playing past 35 which explains the higher standard errors, but they are of a higher caliber than any other age group which is why they are still playing. Thus, we see their proportion as higher than we would expect, given the trend of other 30-35 year old players.

# Literature Review {#sec-LitReview}

As mentioned, Fellingham and Fisher (2017) have both considered and published their thoughts concerning prediction of player home run performance using Bayesian multilevel modeling. To do this, Fellingham and Fisher (2017) used data from the Lahman data set spanning 1871 to 2008 to estimate the parametric weights for their model. Over this time frame they only considered players who had played for at least 6 seasons and who, in each of those seasons, had at least 50 AB's. They further restricted their pool of players to only considered seasons where the players were 18 years old to 45 years old.

To predict a player's HR's they used a binomial logistic model. This model predicts HR's using the AB's of a player for the number of trials and the underlying ability of a player to hit a HR, $\pi$, as their probability of success. Fellingham and Fisher (2017) estimate a player's ability to hit home runs using his age in a season, decade born, season played, and home ball park as predictors. This leaves Fellingham and Fisher (2017) with the following model, which we have adapted to make their model's parameter notation match our model's notation (in @sec-Model):

$$
\begin{align}
HR_{nip}|\pi_{nip}&\sim Binomial(AB_{nip},\pi_{nip})\\
\\
\log(\frac{\pi_{nip}}{1-\pi_{nip}})&=[\beta_{n1}\cdot Age_{ni}]+[\beta_{n2}\cdot(Age_{ni})^2]+[\beta_{n3}\cdot(Age_{ni})^3]+[\beta_{n4}\cdot(Age_{ni})^4]\\
&+\alpha_n+\xi_i
+\delta_p
\end{align}
$$ {#eq-FFmodel}

Where $HR_{nip}$ is the HR's hit by a player $n$ in year $i$ played at stadium $p$. $AB_{nip}$ is the AB's for player $n$ in year $i$ at stadium $p$. $\pi_{nip}$ is the probability of a HR in an AB for player $n$ in year $i$ at stadium $p$ measured on the logistic scale.

The parameter's $\beta_{n,1-4}$ describe the weights of the orthogonal polynomial, up to the quartic degree, for the ages of the players. We theorize, because Fellingham and Fisher (2017) did not explicitly articulate why, they chose a quartic model because it has a closer fit to the data displayed in @fig-HR.By.Age.All and @fig-HR.By.Age.20to40. The properties of a squared term capture the rise and fall of player's HR probability over their career. The inclusion of the cubic term allows for the modeling of two peaks, while the quartic term allows it to maintain that overall trend of up and then down seen in the data. This appears in @fig-HR.By.Age.20to40 where players see a steady increase in their HR probability during their career with a peak when they are 27. Then, their probability decreases a little, followed quickly by another increase, which then leads to another peak when the player is 31. Afterwards, the probability continues in a downward trend. This will be discussed in more depth in @sec-Model but our model, which aims to improve Fellingham and Fisher's (2017) model, does not account for the cubic or quartic effects due to fear of over-fitting the age data, since the distinction between the humps is rather small.

The parameter $\alpha_n$ (which is $\theta_i$ in the actual paper) are the individual intercepts for each player. This parameter is the only one that Fellingham and Fisher (2017) use which takes advantage of multilevel modeling. In this modeling, Fellingham and Fisher (2017) set the mean of $\alpha_n$ to be $\mu_d$ where $d$ is the decade the player was born. That means that Fellingham and Fisher (2017) set the parameter which measures the player's "innate" HR probability based on the HR probability of other players who were born in the same decade. This allows Fellingham and Fisher (2017) to make confident predictions about players who may only have 6 seasons of data because this model borrows data from all players who were born at around a similar time. Implicitly, this means that Fellingham and Fisher (2017) assume that when a player was born is a powerful predictor of their HR probability.

In contrast, the parameter $\xi_i$ (which is $\delta_i$ in the actual paper) represents the effect that season $i$ has on player $n$'s ability to hit a HR at stadium $p$. This $\xi$ is calculated for each season considered where $i\in\{1,…,138\}$ for the 138 seasons considered. This $\xi_i$ is calculated independent of each player so it represents the different changes that we see in HR ability over all years considered. An example of why this effect is important refer to @fig-HRByYear, which clearly shows how HR rates have differed drastically over the years considered. Although, the inclusion of $\xi_i$ is confusing because it probably captures the same effect that $\alpha_n$ captures. Most players in the MLB are within 10 years of age of each other with the most extreme different in age being about 20 years. Thus, the $\alpha_n$ of players is unlikely to differ by very much and any difference in $\alpha_n$ which appears is likely also apparent in $\xi_i$. Thus, the inclusion of both $xi_i$ and $\alpha_n$ is strange because it introduces a strong possibility of multicollinearity since both parameters describe roughly the same trend in the data.

The parameter $\delta_p$ (which is $\xi_p$ in the actual paper) is the effect that playing in park $p$ has on the HR probability $\pi_{nip}$. As discussed in @sec-BaseBack, there is strong precedent to include such a term because the variance between ballpark dimension and environment can be drastic. Fellingham and Fisher (2017) estimate the effect that each park has on a player's ability to hit a HR and then add the associated effect to the players who played at that ballpark in a given year.

One of the main reasons that Fellingham and Fisher's (2017) model is so strange is because of the priors that they chose. Disregarding that there is no justification for these priors in their paper, Fellingham and Fisher (2017) set the following priors for these parameters:

$$
\begin{align}
\alpha_n&\sim Normal(\mu_{d_n},\sigma^2_{d_n}), d\in \{1...14\}, n\in\{1,...,3735\}\\
\\
\mu_d&\sim Normal(-2.5,1), d\in\{1,...,14\}\\
\\
\sigma^2_{d}&\sim Gamma(3,3), d\in\{1,...14\} \\
\\
\xi_i&\sim Normal(-2.5, 2),i\in \{1,...,138\}\\
\\
\delta_p&\sim Normal(-2.5, 2), p\in \{1,...,222\}\\
\end{align}
$$ {#eq-FFpriors}

These priors are quite strange because they are so uninformative. For example, consider the priors on either $\xi_i$ or $\delta_p$. The priors state that on a logistic scale, it is normal for the decade of birth and season of play parameters to vary along the logistic interval \[-6.5, 1.5\]. On the normal probability scale, this corresponds to probability fluctuations of about (0.0015, 0.81). Furthermore, if you only consider the means that Fellingham and Fisher have used here, then you would be left with a player who, on the logit scale, hits HR's at a probability of -7.5. On the normal probability scale this becomes about 0.00055, which is 0.055% chance any AB ends in a HR. Confusingly, this is not even close to the 2.8% that Fellingham and Fisher (2017) noted in their paper. Surely there must be better priors than this which Fellingham and Fisher (2017) could have used since it does not seem that even they believe these priors that they have set. Coincidentally, this is probably why in using MCMC to estimate their parameters they needed to use 200,000 iterations. Likely, their model could have estimated their parameters much more easily had they given it stronger priors. This is one of the areas this paper seeks to improve on from Fellingham and Fisher's (2017) methods.

Another questionable choice that Fellingham and Fisher (2017) make is that they estimated their model parameters using FORTRAN. Given the antiquity of FORTRAN, replication of Fellingham and Fisher (2017) is quite difficult. Not to mention that the 200,000 iterations makes the research so computationally expensive that it is nearly impossible for any other research team to replicate. This is disappointing because the scientific method relies on replication for the advancement of knowledge. Thus, our research seeks to make the algorithms used in this research more accessible because we chose to use R to interface with the modeling software Stan, which is written in the (more contemporary) C++ programming language. Furthermore, our model will use less iterations and stronger priors so that other research teams can more easily replicate our findings.

Finally, we see that Fellingham and Fisher (2017) project the performance of a number of players from their considered 3,735. However, upon closer inspection these players do not seem to be randomly selected because the selected players were pretty renown during their prime. Nor, does Fellingham and Fisher (2017) make the assurance that these players are randomly selected. This should leave readers with unease because it is unclear how well their model fits most players, which seemed to be what the paper was trying to investigate. Although unlikely, it is conceivable that they only selected players which make their model appear the most reliable. Instead, our paper seeks to remedy this by considering a random assortment of players to examine the fit of our model.

# Model {#sec-Model}

Given all the previous problems with Fellingham and Fisher (2017) mentioned above, there is an argument to be made that a simple generalized linear model (GLM) could be used instead of their complicated Bayesian semiparametric model. While it is true that a GLM could be used to predict HR's of a player it would not nearly be as accurate as a Bayesian model. This is because the greatest strength of using Bayesian modeling is that it allows for multilevel models to be created. That is, Bayesian models allow "learning" from clusters (sometimes known as "pools") of observations when there is not as much data available on a particular observation. Our model employs this method much more liberally than Fellingham and Fisher (2017). In turn, this should allow for us to make stronger inferences than Fellingham and Fisher (2017) because of this "learning".

Exploring concretely how our model works will make it clearer than discussing it in the abstract. The current model that we have is the following:

$$
\begin{align}
HR_{nip} &\sim Binomial(AB_{nip},\pi_{nip})\\
\log(\frac{\pi_{nip}}{1-\pi_{nip}}) &= \alpha_{n}+\beta_n\cdot (Age_{ni}-30)+\eta_n\cdot (Age_{ni}-30)^2+\delta_p+\xi_i
\end{align}
$$ {#eq-model}

$$
\begin{align}
\alpha_n&\sim Normal(\mu_0, \sigma_0),n\in\{1,...,657\}\\
\beta_n&\sim Normal(\mu_1, \sigma_1),n\in\{1,...,657\}\\
\eta_n&\sim Normal(\mu_2, \sigma_2),n\in\{1,...,657\}\\
\delta_p&\sim Normal(\mu_5, \sigma_5),p\in\{1,...,88\}\\
\xi_i&\sim Normal(\mu_6, \sigma_6),i\in\{1,...,47\}\\
\end{align}
$$ {#eq-priors}

$$
\begin{align}
\mu_0 &\sim Normal(-3.5, 0.1)\\
\sigma_0&\sim Exponential(1)\\
\\
\mu_1&\sim Normal(0,0.1)\\
\sigma_1&\sim Exponential(10)\\
\\
\mu_2&\sim Normal(0,0.01)\\
\sigma_2&\sim Exponential(100)\\
\\
\mu_5&\sim Normal(0,0.01)\\
\sigma_5&\sim Exponential(10)\\
\\
\mu_6&\sim Normal(0,0.25)\\
\sigma_6&\sim Exponential(10)
\end{align}
$$ {#eq-hyperpriors}

In this model, we say that the $HR$'s of player $n$ follow a binomial distribution with number of trials, for player $n$ in season $i$ played at park $p$, are $AB_{nip}$. The probability of success (hitting a HR) is denoted as $\pi_{nip}$ for player $n$ in season $i$ playing at park $p$. Furthermore, we estimate $\pi$ on the logistic scale.

As discussed in @sec-BaseBack, there are many factors that influence player $n$'s underlying probability of hitting a $HR$ in a given $AB$. The first parameter which affects this probability, $\pi_{nip}$, is $\alpha_n$. In our model $\alpha_n$ can generally be thought of as the intercept term which is used to account for a player's innate ability to hit HR's. From our exploratory observations of the data, we noticed that the average (median or mean) proportion of AB's which resulted in HR's is about 0.03. This means that about 3% AB's result in HR's. This can be seen in @fig-HR.By.Age.20to40, @fig-pDist, and @fig-pDistSmall. On the logit scale, which is what our model relies on for estimation, a probability of 0.03 is about -3.5. We recognize that some players, for whatever reason, have better "innate" abilities to hit HR's than others. Therefore, we used a multilevel modeling technique for $\alpha_n$ to capture that player-to-player difference. To begin capturing this difference in $\alpha_n$ across players, we set the hyper-priors for $\alpha_n$ to be $\mu_0$ and $\sigma_0$. Where $\mu_0\sim Normal(-3.5,0.1)$ and $\sigma_0\sim Exponential(1)$. We set the mean of $\mu_0$ to be -3.5 because we estimate $\alpha_n$ on the logistic scale and we want it to be close to the average player's value. Since the parameters of $\alpha_n$ are distributions, we can think of $\alpha_n$ as a set of distributions, for the different values that either $\mu_0$ or $\sigma_0$ could take on. Further, the likelihood of the particular distribution that $\alpha_n$ takes on is dependent on the likelihood of either $\mu_0$ or $\sigma_0$ taking on a particular value. For example, $\mu_0=-3.0$ is extremely unlikely because it lies four standard deviations away from the mean of its normal distribution -3.5. Thus a distribution for $\alpha_n$ being $Normal(-3,\sigma_0)$ is highly unlikely for any value of $\sigma_0$. We chose $\mu_0$ to be normally distributed because there is precedent in Fellingham and Fisher (2017) to use normal distributions for their mean hyper-priors. Another reason it is easier to interpret normal distributions. We chose $\sigma_0$ to be exponentially distributed because the exponential distribution does not allow for negative values and the bulk of the possible values of $\sigma_0$ would be clustered near 0. Since we cannot have negative values for a standard deviation it seemed appropriate. This use of these hyper-priors is what makes the model appear to "learn". The model already comes in with certain expectations of what values $\alpha_n$ should take on, rather than simply coming into each new $n$ knowing nothing, which is what happens with more traditional GLM's. Finally, with this new set of distributions for $\alpha_n$ being the priors, our model constructs a posterior using Bayes theorem and the data on player $n$. It repeats this process for all 657 players that we considered for this research question to create a different $\alpha$ distribution for each player, $n$. Whatever value in $\alpha_n$ has the maximum likelihood, that's what we used to generate our predictions for inference in @sec-Results.

The next parameter we considered which affects $\pi_{nip}$ was $\beta_n$ which represents the effect centered age (the player's age minus 30) had on player $n$'s ability to hit HR's. This topic was briefly touched upon in @sec-BaseBack, where we discussed how a player's age can impact their their strategy and physical prowess. These ideas are well demonstrated in @fig-HR.By.Age.20to40. In @fig-HR.By.Age.20to40, one can easily see that, in general, players' ability to hit HR's improves over the course of their career and peaks when players reach about 30 years old. After this peak, a player's ability starts to decline for the rest of their career. We chose to center the the age of the all players at 30 for several reasons. The first was that roughly the mean and the median ages in the data was 30. Second, having a centered age makes interpreting $\beta_n$ easier. The value of $\beta_n$ represents how changes away from the centered age affects player $n$'s probability of hitting a HR, $\pi_{nip}$. In a normal GLM's, $\beta_n$ would not be able to adapt to each player, $n$, as it does in our model. It would only be able to consider how aging affects the average player's HR probability and then apply that effect to all players. This implicitly assumes that each player is affected identically by changes in age. In contrast, we employ multilevel modeling techniques to our model where $\beta_n\sim Normal(\mu_1,\sigma_1)$, $\mu_1\sim Normal(0,0.1)$, and $\sigma_1\sim Exponential(10)$. Similar to understanding $\alpha_n$, we can interpret $\beta_n$ as a collection of distributions corresponding to the different values of its hyper-priors $\mu_1$ and $\sigma_1$ and their corresponding likelihoods. We chose a normal distribution for $\mu_1$ for reasons similar to $\mu_0$. It makes interpretation easier and there is precedent in the literature. Furthermore, we chose it to have a mean of 0 because we assume that there is only a slightly positive increase in HR probability year over year, which is close enough to zero that we let the data speak for itself. Again, for similar reasons to $\sigma_0$ we chose $\sigma_1$ to be exponentially distributed. This ensured that $\sigma_1$ would not be negative and that the bulk of the possible values of $\sigma_1$ would be clustered near 0. With the distributions calculated for $\beta_n$ we can now use $\beta_n$ as the prior and combine it with the player data to generate the posterior using Bayes theorem. We repeat this process for all 657 players considered to get a unique distribution for each player $\beta_n$. This means that each player, $n$ has a unique aging coefficient which represents how aging effects their HR probability. Whatever value in $\beta_n$ has the maximum likelihood, that's what we used to generate our predictions for inference in @sec-Results.

Similarly $\eta_n$ is the effect that player $n$'s squared age has on his probability of hitting a HR in an AB. The purpose of this squared term is to capture the non-linear relationship between hitting a HR and player $n$'s age. It is apparent in both @fig-HR.By.Age.20to40 and @fig-HR.By.Age.All that there is a parabolic relationship between age and player $n$'s ability to hit HR's. This ability appears to rise steadily for the average player until it reaches its apex when the player is roughly 30 years old. Then, in the seasons where the player is more than 30 years old we see a decline in HR probabilities. The shape we are left with, as can be seen in @fig-HR.By.Age.20to40, is one of an upside-down parabola. Thus, by including this squared age effect, we are able to capture that unique relationship. However, as one can easily notice in @fig-HR.By.Age.20to40, the overall shape is parabolic but for ages past 37 the data begins to trend upwards. Likely, this trend upwards is due to players who have above average skill remaining in MLB beyond their average counterparts. That is, players who have earned massive contracts because they were so much better than the average player in their prime earned large, multimillion dollar, contracts and teams do not want to waste the this money so they play these players. So, we are then left with this group of outlying players who do not follow the trend of data at any point over their career, but it only became noticeable after the other, more normal, players disappeared from the considered sample.

Returning to the parameter $\eta_n$, it is nearly impossible to interpret what its meaning because it describes the relationship between centered age squared and HR probability. Centered age squared on its own is already almost a meaningless concept. Therefore, it is best understood that this term simply accounts for the non-linearity we see in the data and not become too concerned with interpreting its estimates. However, it is easy to justify our choice for the priors and hyper-priors of $\eta_n$, $\mu_2$, and $\sigma_2$, respectively. We chose $\eta_n\sim Normal(\mu_2,\sigma_2)$ for the same reasons discussed for both $\alpha_n$ and $\beta_n$: we wanted a multilevel model which described how individual players were impacted by the parameters. We chose $\mu_2\sim Normal(0,0.01)$ because we assumed that there would be a negative effect so small that it would be very close to zero. Further, any distinction to the slightly negative of zero would be hard to justify and, in essence, arbitrary. Therefore, we went with the mean of 0 and the standard deviation of 0.01 to account for any players who have a more or less extreme non-linearity to their trajectories. That is, higher or lower peaks and troughs throughout their career for their probability $\pi$. We chose $\sigma_2\sim Exponential(100)$ because standard deviations cannot be negative, which the exponential distribution lends itself nicely to. Furthermore, the exponential distribution with a parameter of 100 is very concentrated near 0. Since we know that the coefficient of $\eta_n$ is likely to be very small since it is being multiplied by the age squared (which can be over 100). Thus, we can be confident that our standard deviation will end up being small for our estimate. This means that the estimates our model generates for $\eta_n$ will be distributions highly concentrated near 0 until the data for player $n$ informs otherwise through Bayes theorem. This leaves that each player, $n$ with another unique aging coefficient which represents how non-linear aging effects their HR probability. Whatever value in $\eta_n$ has the maximum likelihood, that's what we used to generate our predictions for inference in @sec-Results.

Another parameter of our model is $\delta_p$, which describes the effect playing in ballpark $p$ has a player's HR probability. There are 88 different ballparks we considered, and we generated an estimate for each of them. This was discussed in @sec-BaseBack, where we highlighted that players in certain ballparks are more likely to hit HR's than in others because of the different dimensions of the outfield fences and the different altitude of the parks. Furthermore, we know that these parks will effect players differently because teams play at different ballparks with varying frequency. For example, players will play 81 of their games at their home-stadium and 81 at various other ballparks known as "on the road." Thus, the home-stadium that player $n$ plays at is an important factor for their HR probability, $\pi$, because player $n$ plays many more games at his home-stadium compared to any other ballpark. In our model, we have estimated $\delta_p$ as an intercept term and it takes advantage of multilevel modeling since we have hyper-priors, $\mu_5$ and $\sigma_5$. We have purposely set the hyper-prior of $\mu_5\sim Normal(0,0.01)$ with mean 0 because we assume that parks in the aggregate will not have an effect on a player's ability to hit HR's. There is no formal justification for this, it is just an assumption that we make based on our preliminary exploration and our understanding that ownership (who controls the dimensions of the stadium) are not particularly biased to any ballpark size or location which means that we should see good variation between parks. We chose a standard deviation for $\delta_p$ to be $\sigma_5\sim Exponential(10)$ for similar reasons to all of the other $\sigma_i$'s. The exponential distribution cannot be negative, which we need for standard deviations. Further, we know that the effect of each park will probably not be huge because, although there is deviation, there is not extreme deviation between parks that makes some "unfair" to play at. With the distributions calculated for $\delta_p$ (similar to every other multilevel parameter) we can now use $\delta_p$ as the prior and combine it with the player data to generate the posterior using Bayes theorem. We repeat this process for every park $p$ to generate the posterior distribution of possible effects that playing at park $p$ has on a player's HR probability. Whichever value in $\delta_p$ has the maximum likelihood, that is what we use to generate our predictions for inference in @sec-Results.

Our final parameter, $\xi_i$ represents the change in the underlying probability of hitting a HR in the year $i$. This stems mostly from the observation in @fig-HRByYear which shows a steep increase in the amount of HR's hit per year across any subset of years considered. Further, we can tell from @fig-pByYear that these additional HR's are not just the product of expansion teams creating more opportunities for players to hit more HR's. @fig-pByYear shows that the proportion of AB's which are HR's have risen steadily over the period considered which suggests that players' probabilities have been changing. This means that there is an underlying change in the $\pi$ depending on the year considered. Regardless of the reason for this change, we can see that there is an increase and we need to account for this in our estimates for $\pi$ for player $n$. Thus, we include $\xi_i$ to represent the effect that playing in year $i$ will have on a player's ability to hit a HR. We set the hyper-prior of $\xi_i$ as $\mu_6\sim Normal(0, 0.25)$ because we can see that the HR proportion change year over year in @fig-pByYear by only fractions of a percent. Thus, when we set our prior with mean 0, we are saying that we expect most year over year variation to be low and we would rather have the data speak through in the posterior compared to use setting a negligibly small prior. Furthermore, we set the hyper-prior $\sigma_6\sim Exponential(10)$ for the same reasons mentioned during the other $\sigma_i$'s. The exponential distribution ensures that we never have a negative estimate for our standard deviation of $\xi_i$, and having a parameter value of 10 for $\sigma_6$ allows the data to be fairly concentrated near 0 which is what we would expect for a year over year effect. With the distributions calculated for $\xi_i$ (similar to every other multilevel parameter) we can now use $\xi_i$ as the prior and combine it with the player data to generate the posterior using Bayes theorem. We repeat this process for every year $i$ to generate the posterior distribution of possible effects that playing in year $i$ has on a player's HR probability. Whichever value in $\xi_i$ has the maximum likelihood, that is what we use to generate our predictions for inference in @sec-Results.

Compared to most GLM's, this model ([@eq-model]) we have created is massive. It has 2,116 parameters. It has 657 different $\alpha, \beta,$ and $\eta$'s estimated for each player. It has 88 $\delta$'s for each park and 47 $\xi$'s for each year. Finally, it has 10 different $\mu$'s and $\sigma$'s for the different hyper-priors. All together, totaling 2,116 parameters. This gargantuan size is what allows such powerful inference for our model. Even though the data on each player individually is sparse, since there is so much data available for the model to take in beyond that player and the model uses many simulations to create estimates, this allows our model to make strong inferences concerning future performance.

# Results {#sec-Results}

The goal of our model, as described in @eq-model, @eq-priors, and @eq-hyperpriors, was to be able to adaptively predict player $n$'s HR's. That is, we wished to create a model that would predict player $n$'s HR's and would adapt these predictions to be unique to player, $n$, the park played at, $p$, and the year played, $i$. To check the fit of their model, Fellingham and Fisher (2017) selected popular players (such as Hank Aaron and Barry Bonds) as a representative sample of the player population. However, the problem with this approach is that Fellingham and Fisher (2017) have selected players who are phenomenal hitters and generally outliers in the data. For example, Barry Bonds currently holds the single season and career HR records. Which means that he has the most HR's hit in one season and the most hit over the course of a career compared to all people who have ever played in MLB. The career HR record he holds was previously held by Hank Aaron, who now sits second on the list. Obviously, this sample of players is not representative of the population of players considered by Fellingham and Fisher (2017). So, while it may make the paper more enjoyable to read, it does not provide use with a rigorous understanding of the general fit of this model. We believe that a random sample should have been used to examine the fit of their model since it would give a better sense of overall model fit. Which, is exactly what we did in this paper. From the 657 players considered, we used R to select four random players to see how our model both fits and adapts to the different careers of players. These players are: Robin Yount ([@sec-yount]), Pat Tabler ([@sec-tabler]), and Kendrys Morales([@sec-morales]).

In exploring the efficacy of our model, we chose to examine $\pi_{nip}$ for the players mentioned above instead of $HR_{nip}$. We did this because moving between the expected value of HR's ($E(HR_{nip})$) and the HR probability ($\pi_{nip}$) is rather elementary due to $E(HR_{nip})=AB_{nip}\times\pi_{nip}$, since $HR_{nip}$ is a binomial random variable. We assumed when making our model that $AB_{nip}$ were given which means moving between $E(HR_{nip})$ and $\pi_{nip}$ is simply a matter of multiplication or division depending on which term you would like to reference. Thus, because of the similarity between what $E(HR_{nip})$ and $\pi_{nip}$ are describing we decided to only focus on $\pi_{nip}$ without loss of generality.

Furthermore, we chose to examine $\pi_{nip}$ relative to centered age for all player. This is because centered age corresponds directly to the parameters $\beta, \eta,$ and $\xi$ in our model ([@eq-model]). It should be obvious why centered age corresponds to $\beta$ and $\eta$. However, the reason we know that centered age corresponds to $\xi$ directly is because players are only an age for one season. Thus, if we are given a year and player $n$'s corresponding age we can deduce the year for all ages of the player. This means that the year-over-year effect, which $\xi$ captures, is directly present in the figures. The parameters $\alpha$ and $\delta$ work more in the background of the predictions presented in the figures. We leave the discussion of each to when they appear in the figures of certain players.

## Data

Similar to Fellingham and Fisher (2017), we have used the Lahman data set for player statistics. While Lahman has data on a wide range of MLB related topics, we decided to focus on a singular data sheet. We focused on the "Batting" data set in Lahman which describes player-year-team batting statistics. That is, each entry in Batting is for a player $n$ who played in year $i$ for team $t$. This allows multiple entries for the same year for the same player if they were traded, which is important for our analysis because different teams implies different home ballparks.

In order to be considered in our analysis a player had to be in Batting and have played for at least 6 seasons from 1973-2019. We chose these cut-offs because in 1973 the American League allowed the Designated Hitter, signaling that there would be an increased specialization in labor for hitting. We chose 2019 as the other cut-off because 2020 was the COVID season which was drastically shortened and very different compared to almost all other previous seasons. Furthermore, in order to be considered in our analysis players had to have had at least 50 AB's across those 6 seasons played. The effect of these cut-offs was that we were left with players who had longer careers and who hit HR's. There were many players dropped who never hit HR's because of their short stints which made prediction hard and prediction for these players did not seem fruitful given their general inconsequential careers.

```{r}
#| eval: false

#Since we aren't fitting anything when we render, we don't need this chunk to take up any extra time calculating usless vectors
data.poly.fit <- list(
  N = nrow(Batting),
  n_player = n_distinct(Batting$player_index),
  n_park = n_distinct(Batting$park_index),
  n_year = n_distinct(Batting$yearID),
  AB = Batting %>% pull(AB),
  HR = Batting %>% pull(HR),
  Player = Batting %>% pull(player_index),
  cAge = Batting %>% pull(cAge),
  cAgeSq = (Batting %>% pull(cAge))**2,
  Park = Batting %>% pull(park_index),
  Year = Batting %>% pull(yearID) -1972
)
```

```{r}
#| eval: false

#We don't want the program, which takes my desktop 30-45 minutes to render, to run every time I need to render the docuement
fit.poly <- stan(
  file = "HRPolyFit.stan",
  data = data.poly.fit,
  iter = 1e4
)
```

```{r}
#| eval: false

#This saves the fit as and RDS so that I can reference it later when rendering
saveRDS(fit.poly, "PolyFit.rds")
```

```{r}
#Lets me load in the model that I made. Will only work on my desktop.
#fit.poly <- readRDS("C:/Users/Sam/Desktop/RDS/PolyFit.rds", )

#Lets me load in the model for my Mac
fit.poly <- readRDS("~/Desktop/RDS/PolyFit.rds")
```

```{r}
#| eval: false
plot(fit.poly, pars="delta")
```

```{r}
#| eval: false
print(fit.poly, pars="delta")
```

```{r}
#Using the weights of the model, lets predict how a couple of players would do and compare it to how they actually did

#We want to set the seed of this chunk so that we're talking about the same players everytime we run the chunk
set.seed(1)

#This gives us a list of 10 players to consider so if there are a couple that look similar we can choose to not include them in the analysis
select_players <- Batting %>% 
  distinct(playerID) %>% 
  pull(playerID) %>% 
  sample(size=10, replace=FALSE)
```

```{r}
select_players_Batting <- Batting %>% 
  filter(playerID %in% select_players) %>% 
  select(playerID, player_index, yearID, year_index, Age, cAge, cAgeSq, AB, HR, p, park_index)

#select_players_Batting
```

```{r}
#As a warning to all who are going to be running this in the future: even calculating the parameters from the model (alpha, beta, eta, and xi) are computationally expensive. They take a good chunk of time to calculate which makes rendering the document take a couple minutes.
```

## Robin Yount {#sec-yount}

```{r}
set.seed(1)

select_player <- Batting %>% 
  distinct(playerID) %>% 
  pull(playerID) %>% 
  sample(size=1, replace=FALSE)

select_player_Batting <- Batting %>% 
  filter(playerID %in% select_player) %>% 
  select(playerID, player_index, yearID, year_index, Age, cAge, cAgeSq, AB, HR, p, park_index)
```

```{r}
# Robin Yount (651) Estimates cell

alpha_i651 <- fit.poly %>% 
  spread_draws(alpha[group]) %>% 
  summarise_draws() %>% 
  filter(group==651)

beta_i651 <- fit.poly %>% 
  spread_draws(beta[group]) %>% 
  summarise_draws() %>% 
  filter(group==651)

eta_i651 <- fit.poly %>% 
  spread_draws(eta[group]) %>% 
  summarise_draws() %>% 
  filter(group==651)

#Meanwhile, this value corresponds to the park
delta_i651 <- fit.poly %>% 
  spread_draws(delta[group]) %>% 
  summarise_draws() %>% 
  filter(group==24)

#And this value corresponds to the years that he played
xi_i651 <- fit.poly %>% 
  spread_draws(xi[group]) %>% 
  summarise_draws() %>% 
  filter(group >= 2 & group <= 21)
```

```{r}
select_player_Batting <- select_player_Batting %>% 
  mutate(alpha = alpha_i651 %>% pull(mean), .after = player_index) %>%
  mutate(beta = beta_i651 %>% pull(mean), .after = cAge) %>% 
  mutate(eta = eta_i651 %>% pull(mean), .after = cAgeSq) %>% 
  mutate(delta = delta_i651 %>% pull(mean), .after = park_index) %>% 
  mutate(xi = xi_i651 %>% pull(mean), .after = year_index) %>% 
  mutate(pred = alpha + (beta*cAge) + (eta*cAgeSq) + (delta) + (xi) , .after = p) %>% 
  mutate(inv_logit_pred = inv.logit(pred), .after=pred)
```

Consider player $n=651$. That is, consider the player Robin Yount. In @fig-p651Act we can see how Yount's actual HR probability has changed over the years that he played. The x-axis of @fig-p651Act describes Yount's deviation from the centered age, where the centered age value of 0 represents a player being 30 years old. And the y-axis represents the actual HR probability of Yount. Thus, the point (0, 3.25) tells us that Yount's actual HR probability was 3.25% when he was 30 years old. Similarly, the points (-10, 1.5) and (6, 2) represent Yount's actual HR probability being 1.5% when he was 20 years old and 2% when he was 36 years old, respectively. Included in @fig-p651Act is a blue trend line that snakes through the data. This line does not represent anything meaningful, it was included simply to make seeing and interpreting the trend present in the scatter-plot easier. To that end, we can see in @fig-p651Act that during the first 10 years of his career, Yount saw a steady increase in his actual HR probability until he was 28 years old. From when he was 29 years old until he was about 35 years old, there some fluctuation in his HR probability by overall it stayed fairly consistent at around 2.5-3%. Finally, in the last three years of his career we can see Yount's HR probability decline to about 1.75% when he was 38 years old. Overall, Yount appears to have a typical career compared to what we told our model to expect. There was a steady increase in probability until he was about 30, followed by a decline afterwards as he got older. The shape of the trend-line through the scatter-plot follows a vaguely parabolic shape which is what we would expect to see. Thus, Yount will be a good benchmark for the model to see how it predicts "typical" players.

In @fig-p651Pred we can examine how our model predicted Yount's HR probability, $\pi$, to change over his career. The x-axis is the same as it is in @fig-p651Act, where it describes Yount's deviation from the centered age (which is 30). The y-axis describes Yount's predicted HR probability $\pi$. An important note is that the y-axis is not measured on the logistic scale. We calculated it on the logistic scale but then converted it back to the normal scale, and use the normal scale in the figure, to prevent any confusion. Interpreting the points in @fig-p651Pred is very similar to the interpretation of points for @fig-p651Act. Each point in @fig-p651Pred represents the model's prediction of Yount's $\pi$ for a centered age value. For example, the point (0, 3.25) represents the model predicting that Yount's $\pi=3.25$ when he is 30 years old, which was very close to the actual value. Similarly, the points (-10, 1) and (6, 2) represent our model's prediction that Yount's $\pi=1$ when he is 20 years old and $\pi=2$ when he is 36 years old, respectively, which is also very close to what we saw in @fig-p651Act. Included in @fig-p651Pred is a red trend line for the same reasons mentioned in discussing @fig-p651Act. The red trend line represents the overall trend of the data in the scatter-plot. It is not nearly as necessary here, because the data is so clearly clustered in a parabolic pattern, but it is helpful in future plots. We can see in @fig-p651Pred that the predictions from our model follow closely to what we told it to expect. There is an increase in HR probability, $\pi$, for Yount from when he was 18 to when he was 32. This is quite close to what we see in the figure representing his actual HR probability. Then we see a sharp decline in predicted HR probability for Yount as he ages past 32 until he retires at the age of 38.

Overall, it seems that our model fit Yount's trajectory well. In any given year we could see that the predicted HR probability, $\pi$, was closer to the mean value of 3% than Yount's actual HR probability, but this is a by-product of multilevel modeling and not a defect with the model. Our model is told to assume average and update expectations given past performance. Which is what it did, on any given year it assumes Yount is more average than he really is, for both good and bad years. In addition, since we can see that Yount's actual trajectory and his projected trajectory were quite similar, it seems fair to say that our model did a good job at predicted Yount's HR probability. Of course, this is not unexpected; Yount followed a trajectory that our model should do very well with. There were not many unique features to his career where he did astoundingly well or poorly. Thus, although our model did well with Yount, we expected it to do well because it was not a challenging player to model. It will be interesting to consider other players who do not have as normal of a career.

## Pat Tabler {#sec-tabler}

```{r}
# Pat Tabler (582) Estimates cell

#Find out what parks player 582 played in over his career
i582_park <- Batting %>% 
  filter(player_index == 582) %>% 
  distinct(park_index) %>% 
  pull(park_index)

#Find our what years player 582 played in over his career
i582_year <- Batting %>% 
  filter(player_index == 582) %>% 
  distinct(year_index) %>% 
  pull(year_index)

#Find the all the associated parameters needed for to predict model of player 582
#These first three are specfic estimates for 582
alpha_i582 <- fit.poly %>% 
  spread_draws(alpha[group]) %>% 
  summarise_draws() %>% 
  filter(group==582)

beta_i582 <- fit.poly %>% 
  spread_draws(beta[group]) %>% 
  summarise_draws() %>% 
  filter(group==582)

eta_i582 <- fit.poly %>% 
  spread_draws(eta[group]) %>% 
  summarise_draws() %>% 
  filter(group==582)

#Meanwhile, this value corresponds to the park
delta_i582 <- fit.poly %>% 
  spread_draws(delta[group]) %>% 
  summarise_draws() %>% 
  filter(group %in% i582_park)

#And this value corresponds to the years that he played
xi_i582 <- fit.poly %>% 
  spread_draws(xi[group]) %>% 
  summarise_draws() %>% 
  filter(group %in% i582_year)
```

```{r}
#Create the trajectory DF for player 582
i582.Trajectory <- Batting %>% 
  #Grab the player we're interested in
  filter(player_index == 582) %>% 
  #Only keep columns we're interested in
  select(playerID, player_index, yearID, year_index, Age, cAge, cAgeSq, AB, HR, p, park_index) %>%
  #Put the alpha, beta, and eta parameters in the DF
  mutate(alpha = alpha_i582 %>% pull(mean), .after = player_index) %>%
  mutate(beta = beta_i582 %>% pull(mean), .after = cAge) %>%
  mutate(eta = eta_i582 %>% pull(mean), .after = cAgeSq) %>%
  #Set-up to add "delta" to the table because 582 was traded
  right_join(delta_i582 %>% select(group, mean), by = join_by(park_index==group)) %>% 
  mutate(delta = mean, .after = park_index) %>% 
  select(!mean) %>% 
  #Set-up to add "xi" to the table because 582 was traded
  right_join(xi_i582 %>% select(group, mean), by = join_by(year_index==group)) %>% 
  mutate(xi = mean, .after = year_index) %>% 
  select(!mean) %>% 
  #Create the predictions for the player
  mutate(pred = alpha + (beta*cAge) + (eta*cAgeSq) + (delta) + (xi) , .after = p) %>% 
  mutate(inv_logit_pred = inv.logit(pred), .after=pred)
```

Consider player $n=582$. That is, consider the player Pat Tabler. In @fig-p582Act we can see how Tabler's actual HR probability has changed over the years that he played. The x-axis of @fig-p582Act describes Tabler's deviation from the centered age, where the centered age value of 0 represents a player being 30 years old. And the y-axis represents the actual HR probability of Tabler. Thus, the point (-1, 2) tells us that Tabler's actual HR probability was 2% when he was 29 years old. In fact, the interpretation of @fig-p582Act is exactly the same as it was for @fig-p651Act, expect the probabilities are for Pat Tabler instead of Robin Yount. Observant readers may have noticed that there are two data points when $x=0$ and when $x=2$. This is because Pat Tabler was traded twice, once when he was 30 and another time when he was 32 years old. Since he was traded, it meant that he had two different home ballparks for that season. Thus, he had two different HR probabilities for that year with each representing his HR probability for a specific home ballpark. This "double observation" does not impact our impact our model at all. It is simply a quirk of the data.

Included in @fig-p582Act is a blue trend line that snakes through the data. This line does not represent anything meaningful, it was included simply to make seeing and interpreting the trend present in the scatter-plot easier. To that end, we can see in @fig-p582Act that for the first 7 years of his career Tabler saw an increase in his actual HR probability, which lasted until he was 29 years old. Tabler's average HR probability at this time was about 1.5% during this period. Every year after he turned 29, with the exception of when he was 32, he performed starkly worse than he did in the beginning of his career. The average of these points being around 0.5%. The only outlier in this period was the season where he was 32, where he was traded to the New York Mets and had a higher HR probability for them that year then he did with the Kansas City Royals. However, this higher HR probability was probably due to sample size. He had the same amount of HR's with both teams but more than 4 times as many AB's with the Royals compared with the Mets. Overall, we can see that Tabler's career was different than Yount's in a couple key ways. The first is that Tabler stays well below the average HR probability for all of his career, whereas Yount says very close to it. The second way is that Tabler peaks before he is 30 and does so bad for the final years of his career that there is not much decline. Thus, it will be interesting to see how our adapts to this very interesting and unique career.

In @fig-p582Pred we can examine how our model predicted Tabler's HR probability, $\pi$, to change over his career. The x-axis is the same as it is in @fig-p582Act, where it describes Tabler's deviation from the centered age (which is 30). The y-axis describes Tabler's predicted HR probability $\pi$. An important note is that the y-axis is not measured on the logistic scale. We calculated it on the logistic scale but then converted it back to the normal scale, and use the normal scale in the figure, to prevent any confusion. Interpreting the points in @fig-p582Pred is very similar to the interpretation of points for @fig-p582Act. Each point in @fig-p582Pred represents the model's prediction of Tabler's $\pi$ for a centered age value. For example, the point (-1, 1.55) represents the model predicting that Tabler's $\pi=1.55$ when he is 29 years old. Similarly, the points (-4, 1.2) and (4, 0.8) represent our model's prediction that Tabler's $\pi=1.2$ when he is 26 years old and $\pi=0.8$ when he is 34 years old, respectively. However, the $\pi$ values were not nearly as accurate for Tabler as they were for Yount. We can see that the high variation of the actual HR probabilities has forced our model rely more on the priors than on the actual data. That is, we can see our estimates for $\pi$ are often much closer to the average HR rate across players, which is 3%, than they are to Tabler's specific data because Tabler is such an outlier.

Included in @fig-p582Pred is a red trend line for the same reasons mentioned in discussing @fig-p582Act. The red trend line represents the overall trend of the data in the scatter-plot. Coming into this, the worry was that our model would rely too heavily on its priors and not be able to update its parabolic shape to match the strange career that Tabler had. However, after quickly examining the red line anyone can see that it has clearly updated to fit Tabler's data. That is, we can see that the red line from @fig-p582Pred matches the blue line in @fig-p582Act fairly well, demonstrating that the model updates itself based on the data it is fed. We can see that red line captures Tabler's beginning career well, with a slight increase in HR probability until he is 29. This is then followed by a steep and drastic drop in predicted HR rate for all years including and past when Tabler is 30 years old. Thus, although the actual proportions themselves do not line up as well as we may have hoped, we can see that the model does match trends well, which is a good sign. Thus, it will be interesting to see how different extreme player trajectories challenge the model.

```{r}
#| eval: false
# Junior Felix (177) Estimates cell

#Find out what parks player 177 played in over his career
i177_park <- Batting %>% 
  filter(player_index == 177) %>% 
  distinct(park_index) %>% 
  pull(park_index)

#Find our what years player 177 played in over his career
i177_year <- Batting %>% 
  filter(player_index == 177) %>% 
  distinct(year_index) %>% 
  pull(year_index)

#Find the all the associated parameters needed for to predict model of player 177
#These first three are specfic estimates for 177
alpha_i177 <- fit.poly %>% 
  spread_draws(alpha[group]) %>% 
  summarise_draws() %>% 
  filter(group==177)

beta_i177 <- fit.poly %>% 
  spread_draws(beta[group]) %>% 
  summarise_draws() %>% 
  filter(group==177)

eta_i177 <- fit.poly %>% 
  spread_draws(eta[group]) %>% 
  summarise_draws() %>% 
  filter(group==177)

#Meanwhile, this value corresponds to the park
delta_i177 <- fit.poly %>% 
  spread_draws(delta[group]) %>% 
  summarise_draws() %>% 
  filter(group %in% i177_park)

#And this value corresponds to the years that he played
xi_i177 <- fit.poly %>% 
  spread_draws(xi[group]) %>% 
  summarise_draws() %>% 
  filter(group %in% i177_year)
```

```{r}
#| eval: false
#Create the trajectory DF for player 177
i177.Trajectory <- Batting %>% 
  #Grab the player we're interested in
  filter(player_index == 177) %>% 
  #Only keep columns we're interested in
  select(playerID, player_index, yearID, year_index, Age, cAge, cAgeSq, AB, HR, p, park_index) %>%
  #Put the alpha, beta, and eta parameters in the DF
  mutate(alpha = alpha_i177 %>% pull(mean), .after = player_index) %>%
  mutate(beta = beta_i177 %>% pull(mean), .after = cAge) %>%
  mutate(eta = eta_i177 %>% pull(mean), .after = cAgeSq) %>%
  #Set-up to add "delta" to the table because 177 may have been traded
  right_join(delta_i177 %>% select(group, mean), by = join_by(park_index==group)) %>% 
  mutate(delta = mean, .after = park_index) %>% 
  select(!mean) %>% 
  #Set-up to add "xi" to the table because 177 may have been traded
  right_join(xi_i177 %>% select(group, mean), by = join_by(year_index==group)) %>% 
  mutate(xi = mean, .after = year_index) %>% 
  select(!mean) %>% 
  #Create the predictions for the player
  mutate(pred = alpha + (beta*cAge) + (eta*cAgeSq) + (delta) + (xi) , .after = p) %>% 
  mutate(inv_logit_pred = inv.logit(pred), .after=pred)
```

## Kendrys Morales {#sec-morales}

```{r}
# Kendrys Morales (420) Estimates cell

#Find out what parks player 420 played in over his career
i420_park <- Batting %>% 
  filter(player_index == 420) %>% 
  distinct(park_index) %>% 
  pull(park_index)

#Find our what years player 420 played in over his career
i420_year <- Batting %>% 
  filter(player_index == 420) %>% 
  distinct(year_index) %>% 
  pull(year_index)

#Find the all the associated parameters needed for to predict model of player 420
#These first three are specfic estimates for 420
alpha_i420 <- fit.poly %>% 
  spread_draws(alpha[group]) %>% 
  summarise_draws() %>% 
  filter(group==420)

beta_i420 <- fit.poly %>% 
  spread_draws(beta[group]) %>% 
  summarise_draws() %>% 
  filter(group==420)

eta_i420 <- fit.poly %>% 
  spread_draws(eta[group]) %>% 
  summarise_draws() %>% 
  filter(group==420)

#Meanwhile, this value corresponds to the park
delta_i420 <- fit.poly %>% 
  spread_draws(delta[group]) %>% 
  summarise_draws() %>% 
  filter(group %in% i420_park)

#And this value corresponds to the years that he played
xi_i420 <- fit.poly %>% 
  spread_draws(xi[group]) %>% 
  summarise_draws() %>% 
  filter(group %in% i420_year)
```

```{r}
#Create the trajectory DF for player 420
i420.Trajectory <- Batting %>% 
  #Grab the player we're interested in
  filter(player_index == 420) %>% 
  #Only keep columns we're interested in
  select(playerID, player_index, yearID, year_index, Age, cAge, cAgeSq, AB, HR, p, park_index) %>%
  #Put the alpha, beta, and eta parameters in the DF
  mutate(alpha = alpha_i420 %>% pull(mean), .after = player_index) %>%
  mutate(beta = beta_i420 %>% pull(mean), .after = cAge) %>%
  mutate(eta = eta_i420 %>% pull(mean), .after = cAgeSq) %>%
  #Set-up to add "delta" to the table because 420 may have been traded
  right_join(delta_i420 %>% select(group, mean), by = join_by(park_index==group)) %>% 
  mutate(delta = mean, .after = park_index) %>% 
  select(!mean) %>% 
  #Set-up to add "xi" to the table because 420 may have been traded
  right_join(xi_i420 %>% select(group, mean), by = join_by(year_index==group)) %>% 
  mutate(xi = mean, .after = year_index) %>% 
  select(!mean) %>% 
  #Create the predictions for the player
  mutate(pred = alpha + (beta*cAge) + (eta*cAgeSq) + (delta) + (xi) , .after = p) %>% 
  mutate(inv_logit_pred = inv.logit(pred), .after=pred)
```

Consider player $n=420$. That is, consider the player Kendrys Morales. In @fig-p420Act we can see how Morales' actual HR probability has changed over the years that he played. The x-axis of @fig-p420Act describes Morales' deviation from the centered age, where the centered age value of 0 represents a player being 30 years old. And the y-axis represents the actual HR probability of Morales. Thus, the point (0, 4.25) tells us that Morales' actual HR probability was 4.25% when he was 30 years old. In fact, the interpretation of @fig-p420Act is exactly the same as it was for both @fig-p651Act and @fig-p582Act, expect the probabilities are for Kendrys Morales instead of Pat Tabler or Robin Yount. Observant readers may have noticed that there are two data points when $x=1$ and when $x=6$. This is because Morales was traded twice, once when he was 31 and another time when he was 36 years old. Since he was traded, it meant that he had two different home ballparks for that season. Thus, he had two different HR probabilities for that year with each representing his HR probability for a specific home ballpark. This "double observation" does not impact our impact our model at all. It is simply a quirk of the data.

Included in @fig-p420Act is a blue trend line that snakes through the data. This line does not represent anything meaningful, it was included simply to make seeing and interpreting the trend present in the scatter-plot easier. To that end, we can see in @fig-p420Act that for the first four years of his career Morales saw and increase in his actual HR probability, which lasted until he was 27 years old. Over this period, his actual HR probability rose from 2.25% when he was a rookie, when he was 23 years old, to 6% when he was 27 years old. For the next four years of his career, there was a clear and sharp decline in his actual HR probability. His HR probability dropped from about 5.5%, when he was 28 years old, to 3.1% or 0.5% when he was 31 years old. Then, he saw an average increase in his HR probability to about 5% from when he was 32 years old until he was 35 years old. During the last year of his career, Morales was traded a final time and performed the worst he had done over his career. Between both teams he had a HR probability below 2%. Morales' career has been the most atypical compared to Yount and Tabler. We see him perform astonishingly well to begin and then perform much more poorly in the middle of his career when the average player excels. Then we see him make a recovery towards the end of his career, only for the last year of his career to be quite poor. This makes Morales' career very different than compared to the typical player, such as Yount, so this prediction for Morales will be one of the best tests we can provide to see how our model handles updating posteriors on non-standard data.

In @fig-p420Pred we can examine how our model predicted Morales' HR probability, $\pi$, to change over his career. The x-axis is the same as it is in @fig-p420Act, where it describes Morales' deviation from the centered age (which is 30). The y-axis describes Morales' predicted HR probability $\pi$. An important note is that the y-axis is not measured on the logistic scale. We calculated it on the logistic scale but then converted it back to the normal scale, and use the normal scale in the figure, to prevent any confusion. Interpreting the points in @fig-p420Pred is very similar to the interpretation of points for @fig-p420Act. Each point in @fig-p420Pred represents the model's prediction of Morales' $\pi$ for a centered age value. For example, the point (0, 4.25) represents the model predicting that Tabler's $\pi=4.25$ when he is 30 years old. Similarly, the points (-3, 4.75) and (4, 4.9) represent our model's prediction that Tabler's $\pi=4.75$ when he is 27 years old and $\pi=4.9$ when he is 34 years old, respectively. These $\pi$ values are fairly accurate, similar to what they were for Yount. However, similar to Tabler, we can see that for the seasons that Morales performed extraordinarily well or poorly, our model tends to fit more towards the average than how Morales actually performed. For example, when he was 27 years old Morales had an actual HR probability of 6%, yet our model only predicts for him to hit 4.75%. Similarly, when he was 36 years old Morales actually had a HR probability of less than 2%, yet our model predicts that he would have a HR probability of about 4%. This is likely because those extreme seasons were outliers both for Morales and across the data, so our model has a hard time "believing" that they are true more than they are a fluke. This is one of the properties of multilevel modeling, without a lot of evidence to the contrary it will assume that the player is more average than the observations suggests, which becomes helpful in the long run for prediction.

Included in @fig-p420Pred is a red trend line for the same reasons mentioned in discussing @fig-p420Act. The red trend line represents the overall trend of the data in the scatter-plot. Coming into this, the worry was that our model would rely too heavily on its priors and not be able to update its parabolic shape to match the strange career that Morales had. However, after quickly examining the red line anyone can see that it has clearly updated to fit Morales' strange career. That is, we can see that the red line from @fig-p420Pred matches the blue line in @fig-p420Act fairly well, demonstrating that the model updates itself based on the data it is fed. We can see that red line captures Morales' beginning career well, with an increase in HR probability until he is about 27 years old. This is then followed by a steep and drastic drop in predicted HR rate until he is about 31 years old. Then there is a recovery in $\pi$ until Morales is 34 where we again see a drop in $\pi$ as Morales finishes out his career. Thus, we can see that the model does match trends well. It captures both the peaks that we saw for Morales throughout his career and matches them up to the correct time and gets roughly the same HR probabilities that we saw in the actual data. Therefore, we can be confident in saying that for Morales, our model updates its expectations based on the data well.

## Goodness of Fit - $\alpha$ {#sec-alpha}

There are many numerical measures of goodness-of-fit statistics because researchers are often searching for ways to determine which models, of their many, are the best for their particular research goals. Thus, many of these numerical estimates should be used in a comparative sense, with and against models which are trained on the same data set and attempt to answer the same research question. Examples of these statistics are the Bayesian Information Criterion (BIC), the Widely Applicable Information Criterion (WAIC), the Akaike Information Criterion (AIC), and many others. While it would be possible to quantify these statistics for our model, it would be frivolous. Due to computational constraints, we were only able to generate one model to answer our research question, and it has been the one discussed extensively in this paper. Hence, generating these goodness-of-fit statistics would be useless because they should be used in a comparative context. But, there are no models with which to compare our model to. Our model uses unique data compared to the models which attempt to answer the same research question. Thus, comparing models with these statistics would be useless.

However, that does not mean that there are no ways to check how well the model has fit. Beyond what has been discussed in @sec-yount, @sec-tabler, and @sec-morales, there is another method to check the goodness-of-fit for a Bayesian model. Trace-plots (sometimes called trank-plots) are a way to check how individual parameters have fit. We have generated four plots to explore how the $\alpha$ parameter has fit for four players: Yount (651), Tabler (582), Morales (420), and Junior Felix (177). Junior Felix has not been discussed yet because he was the fourth of the random player's sampled and did not have as interesting of a career as the other three players discussed. Since he did not have a very interesting career, he did not provide much insight into the model fit and updating, so we decided to not display his career trajectory. However, we have included his $\alpha$ parameter estimation @fig-alphaTrace as the last graph in the bottom right because it is a good example of what a trace-plot should look like, which contrasts well with the other trace-plots.

First examine the trace-plot, in the bottom right of @fig-alphaTrace, labeled "alpha\[177\]". This plot represents our models Markov Chain Monte Carlo (MCMC) fitting process. What we should see, if the fit went well, is that the chains (the different colored lines) should remain at roughly the same place over the duration of the iterations. This is what we see in "alpha\[177\]". We can see that, in general, the chains stay between -3.5 and -3 over their 5,000 iterations. There are no sudden deviations so we can conclude that "alpha\[177\]", or $\alpha_{177}$, in our model has a good fit. This stands in contrast to the other fits for $\alpha$. For example, "alpha\[651\]" and "alpha\[420\]" both see a slight upward trend in their $\alpha$ estimates from iterations 5,000 to 7,000. Then after about iteration 7,000 we see a sharp decrease in both estimates. For "alpha\[651\]" this drop goes from about -3.1 to about -3.3. While for "alpha\[420\]" this drop goes from about -2.75 to -3. Furthermore, we can see fluctuation in both "alpha\[651\]" and "alpha\[420\]" from iterations 7,000 to 10,000 suggesting that even after the shift the $\alpha$ parameter still does not find a good fit. The same problems persist for "alpha\[582\]" but are not as pronounced. At around iteration 7,000 our average for "alpha\[582\]" drops from about -4.1 to -4.3. The fluctuations for the average value of "alpha\[582\]" are not as pronounced as they were for either "alpha\[651\]" or "alpha\[420\]", and actually look quite similar to the estimates for "alpha\[177\]".

Overall, we can see that the fit for our $\alpha$ parameter is quite poor for the considered players, even though the our model still adapts to the data well. This suggests that the other parameter fit well since our model did a good job at predicting and adapting to players trajectories. Moreover, in the future if we wish to get a better fit we should consider running our MCMC chain for more iterations. Running for more iterations should allow the model to find the best parameter value, which is what the trace-plots visualize. This is likely why Fellingham and Fisher (2017) ran their model for 200,000 iterations. Allowing the model to iterate for that long will let it find and settle on a parameter value.

## Goodness of Fit - $\beta$ and $\eta$ {#sec-betaandeta}

We can also examine the parameter fits for both $\beta_n$ and $\eta_n$, for $n\in\{651,582,420,177\}$. We chose to group $\beta$ and $\eta$ together in this section because in general they capture the same effect. That effect being how aging impacts player $n$'s ability to hit HR's. Furthermore, one can easily tell from @fig-betaTrace and @fig-etaTrace that the trace-plots for $\beta$ and $\eta$ are not nearly as interesting as they were for $\alpha$ in @sec-alpha.

Consider first the fits for $\beta$ in @fig-betaTrace. The fits presented are for Robin Yount (651), Pat Tabler (582), Kendrys Morales (420) and Junior Felix (177). All fits in this figure look exactly as a good trace-plot should. There are no sudden shifts in the model's estimates for $\beta_n$. Nor are there any serious fluctuations throughout the 5,000 to 10,000 iterations. They all stay at roughly the same value throughout the iterations. We can tell this is not due to scaling on the y-axis because, compared to @fig-alphaTrace, the y-axis of @fig-betaTrace is much more concentrated and has a range of no more that 0.2. Thus, since all the chains stay at roughly the same value over the iterations we know that our model has a good fit for $\beta$ for our four players considered.

Consider second the fits for $\eta$ in @fig-etaTrace. The discussion of these fits will be almost identical to the fits for $\beta$ because the model also fit very well here for the four players considered. The fits presented are for Robin Yount (651), Pat Tabler (582), Kendrys Morales (420) and Junior Felix (177). All fits in this figure look exactly as a good trace-plot should. There are no sudden shifts in the model's estimates for $\eta_n$. Nor are there any serious fluctuations throughout the 5,000 to 10,000 iterations. They all stay at roughly the same value throughout the iterations. We can tell this is not due to scaling on the y-axis because, compared to @fig-alphaTrace, the y-axis of @fig-etaTrace is much more concentrated and has a range of no more that 0.04. Thus, since all the chains stay at roughly the same value over the iterations we know that our model has a good fit for $\eta$ for our four players considered.

Most other Bayesian papers would continue this discussion for their other parameters, which for our model would be discussing $\delta$ and $\xi$. However, these parameters in our model are not player specific, which means to consider them would mean examining 14 $\delta$'s and 33 $\xi$'s. Discussion of of 47 additional parameters in the context of each player seems both trivial and cumbersome. Examining these parameters in the context of the player would not improve our understanding of the model fit. In fact, after examining these parameters we would simply be left with how our model got its estimates which we have already reviewed in @sec-yount, @sec-tabler, and @sec-morales. Thus, because the labor is high and the pay-off is low we have elected to let the reader review any additional goodness-of-fit figures and statistics they would like. The code that we used to generate the model is available on [GitHub](https://github.com/Sturner21/MATH-470) and readers can use it to replicate our findings and investigate our model for themselves.

# Conclusions and Directions for Future Research {#sec-conclusion}

Overall, our model for predicting the HR production of MLB hitters appears to do a good job. It does a fine job at updating its predictions based on the data for each player. But, still keeps its predictions close to the mean unless there is strong evidence, like with Pat Tabler, to suggest that it should deviate. Furthermore, as we can see with the contrast between Robin Yount and Kendrys Morales, our model does not have to rely solely on what the priors tell it. Our model can update itself to either follow a typical career trajectory, as we saw with Robin Yount, and it can update itself to follow an atypical career trajectory, as we saw with Kendrys Morales. Thus, we can conclude that our model does a good job at fitting overall since it appears to take extreme careers in stride.

However, this model is far from perfect. We could see with predictions of $\pi$ for Pat Tabler that our model would not predict HR probabilities $\pi$ at a rate where gambling would be a wise endeavor. These estimates of $\pi$ for many players would be off by enough that the chances of a making money from a bet would be unlikely. But, gambling was never the ultimate motivation behind the prediction. This paper sought to improve on the methods of Fellingham and Fisher (2017), which we believe we did. While it would have been nice to have the computational resources to run this model for 200,000 iterations, we were able to get somewhat stable estimates for our parameters on $\frac{1}{20}$ of the iterations. That is, instead of needing 200,000 iterations to get stable estimates, we were able to get stable estimates using only 10,000 iterations. This suggests that our model is better Fellingham and Fisher's (2017) model at predicting HR's.

Other than having better computational power, this model could be improved with better data. We implicitly assume in this paper that the probability of an AB resulting in a HR remains constant throughout an AB. However, we know that this is not true. A variety of factors that happen during the AB can effect the probability that any given pitch, and thus AB, results in a HR. These factors include, but are not limited to, the type of pitch, the speed of the pitch, crowd noise, state of the game, etc. Having access to this data would be interesting because it would allow our model to update its predictions of a player hitting a HR both before and during an AB. Some of this data is available to the public, and incorporating it into future research is an example of an area where this model could be improved upon.

In the future, this model could easily be adapted to predict other outcomes too, such as hits, walks, strike outs, etc. Any batting outcome would lend itself nicely to this modeling structure. Thus, in the future, it would be interesting to see how this model could be applied to other areas of baseball for prediction.

# Work Cited

Fellingham, G. W., & Fisher, J. D. (2017). Predicting Home Run Production in Major League Baseball Using a Bayesian Semiparametric Model. *The American Statistician*, *72*(3), 253--264. https://doi.org/10.1080/00031305.2017.1401959

Friendly, M., Dalzell, C., Monkman, M., & Murphy, D. (2022). Lahman: Sean 'lahman' 27 baseball database. <https://CRAN.R-project.org/package=Lahman>

McElreath, R. (2020). *Statistical Rethinking: A Bayesian Course with Examples in R and Stan* (2nd ed.). CRC Press.

# Figures {#sec-Figures}

```{r}
#| label: fig-BattingCor
#| fig-cap: "Batting Correlation Table"
corrplot(cor(Batting_cor), type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

```{r}
#Make the dataframe needed for Runs Scored by year graph
Runs.By.Year <- Lahman::Teams %>% 
  group_by(yearID) %>% 
  summarize(R = sum(R))
```

```{r}
#| label: fig-RunsByYear
#| fig-cap: "Cumulative Runs Scored in MLB by Year"

ggplot(data = Runs.By.Year)+
  aes(x=yearID, y=R)+
  geom_line()+
  geom_rect(inherit.aes = FALSE, aes(xmin=1900, xmax=1920, ymin=0, ymax=26000), color="transparent", fill="red", alpha=0.005)+
  geom_rect(inherit.aes = FALSE, aes(xmin=1994, xmax=2004, ymin=0, ymax=26000), color="transparent", fill="orange", alpha=0.005)+
  scale_y_continuous(limits =c(0,26000), breaks = seq(from=0, to=25000, by=5000))+
  scale_x_continuous(breaks = seq(from=1870, to=2020, by=10))+
  labs(title = "Cumulative Runs Scored in MLB by Year",
       x = "Year",
       y = "Runs Scored")
```

```{r}
HomeRuns.By.Year <- Lahman::Teams %>% 
  group_by(yearID) %>% 
  summarize(HR = sum(HR),
            AB = sum(AB)) %>% 
  mutate(p = HR / AB)
```

```{r}
#| label: fig-HRByYear
#| fig-cap: "Cumulative Home Runs in MLB by Year"
ggplot(data = HomeRuns.By.Year)+
  aes(x=yearID, y=HR)+
  geom_line()+
  scale_x_continuous(n.breaks = 20)+
  labs(title = "Cumulative Home Runs in MLB by Year",
       x = "Year",
       y = "Home Runs - League Cumulative")
```

```{r}
#| label: fig-pByYear
#| fig-cap: "Cumulative Home Run Probability in MLB by Year"
ggplot(data = HomeRuns.By.Year)+
  aes(x=yearID, y=p)+
  geom_line()+
  scale_x_continuous(n.breaks = 20)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Cumulative Home Run Probability in MLB by Year",
       x = "Year",
       y = "HR Proportion (HR/AB) - League Cumulative")
```

```{r}
#Make DF which describes the HR by age across the timeline considered
HR.Prop.Age <- Batting %>% 
  group_by(Age) %>% 
  summarize(HR = sum(HR), AB = sum(AB)) %>% 
  mutate(prop = HR / AB, prop.se = sqrt(prop*(1-prop)/AB))
```

```{r}
#| label: fig-HR.By.Age.All
#| fig-cap: "Proportion of Home Runs per At-Bat by Age"

ggplot(data = HR.Prop.Age)+
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - prop.se,
                                ymax = prop + prop.se))+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Proportion of Home Runs per At-Bat by Age",
       x = "Age",
       y = "Proportion of HR per AB")
```

```{r}
#| label: fig-HR.By.Age.20to40
#| fig-cap: "Proportion of Home Runs per At-Bat by Age"

ggplot(data = HR.Prop.Age %>% filter(Age >=20 & Age <= 40))+
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - prop.se,
                                ymax = prop + prop.se))+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Proportion of Home Runs per At-Bat by Age",
       x = "Age",
       y = "Proportion of HR per AB")
```

```{r}
#| label: fig-pDist
#| fig-cap: "Distribution of Players' HR Proportions"
ggplot(data = Batting)+
  aes(x = p)+
  geom_histogram(bins = 75)+
  geom_vline(xintercept=median(Batting$p, na.rm = TRUE) , color="red")+
  scale_x_continuous(n.breaks = 10)+
  labs(title = "Distribution of Players' HR Proportions",
       subtitle = "Seasons Considered: 1973-2019",
       x = "Proportions (p)",
       y = "Observations",
       caption = "Median market in red.")
```

```{r}
#| label: fig-pDistSmall
#| fig-cap: "Constricted View of Distribution of Players' HR Proportions"
#I want this to have a little more breaks in the x-axis to highlight where the median is
ggplot(data = Batting)+
  aes(x = p)+
  geom_histogram(bins = 75)+
  geom_vline(xintercept=median(Batting$p, na.rm = TRUE) , color="red")+
  scale_x_continuous(n.breaks = 16, limits = c(0,0.15))+
  labs(title = "Constricted View of Distribution of Players' HR Proportions",
       subtitle = "Seasons Considered: 1973-2019",
       x = "Proportions (p)",
       y = "Observations",
       caption = "Median marked in red.")
```

```{r}
#| label: fig-p651Act
#| fig-cap: "Actual Trajectory for player #651 - Robin Yount"
ggplot(data = select_player_Batting)+
  aes(x = cAge, y = p)+
  geom_point()+
  geom_smooth(se=FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Actual Trajectory for player #651 - Robin Yount",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in blue.")
```

```{r}
#| label: fig-p651Pred
#| fig-cap: "Predicted Career Trajectory for #651 - Robin Yount"
ggplot(data = select_player_Batting)+
  aes(x = cAge, y=inv_logit_pred)+
  geom_point()+
  geom_smooth(color="red", se = FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Predicted Career Trajectory for #651 - Robin Yount",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in red.")
```

```{r}
#| label: fig-p582Act
#| fig-cap: "Actual Trajectory for player #582 - Pat Tabler"
ggplot(data = i582.Trajectory)+
  aes(x = cAge, y = p)+
  geom_point()+
  geom_smooth(se=FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Actual Trajectory for player #582 - Pat Tabler",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in blue.")
```

```{r}
#| label: fig-p582Pred
#| fig-cap: "Predicted Career Trajectory for #582 - Pat Tabler"
ggplot(data = i582.Trajectory)+
  aes(x = cAge, y=inv_logit_pred)+
  geom_point()+
  geom_smooth(color="red", se=FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Predicted Career Trajectory for #582 - Pat Tabler",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in red.")
```

```{r}
#| label: fig-p177Act
#| fig-cap: "Actual Trajectory for player #177 - Junior Felix"
#| eval: false
ggplot(data = i177.Trajectory)+
  aes(x = cAge, y = p)+
  geom_point()+
  geom_smooth(se=FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Actual Trajectory for player #177 - Junior Felix",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in blue.")
```

```{r}
#| label: fig-p177Pred
#| fig-cap: "Predicted Career Trajectory for #177 - Junior Felix"
#| eval: false
ggplot(data = i177.Trajectory)+
  aes(x = cAge, y=inv_logit_pred)+
  geom_point()+
  geom_smooth(color="red", se=FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Predicted Career Trajectory for #177 - Junior Felix",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in red.")
```

```{r}
#| label: fig-p420Act
#| fig-cap: "Actual Trajectory for player #420 - Kendrys Morales"
ggplot(data = i420.Trajectory)+
  aes(x = cAge, y = p)+
  geom_point()+
  geom_smooth(se = FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Actual Trajectory for player #420 - Kendrys Morales",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in blue.")
```

```{r}
#| label: fig-p420Pred
#| fig-cap: "Predicted Career Trajectory for #420 - Kendrys Morales"
ggplot(data = i420.Trajectory)+
  aes(x = cAge, y=inv_logit_pred)+
  geom_point()+
  geom_smooth(color="red", se=FALSE)+
  scale_y_continuous(labels = label_percent())+
  labs(title = "Predicted Career Trajectory for #420 - Kendrys Morales",
       x = "Centered Age (Player Age - 30)",
       y = "HR's per AB",
       caption = "General trend of data in red.")
```

```{r}
#| label: fig-alphaTrace
#| fig-cap: "Alpha Traceplots"
#651, 582, 177, 420
traceplot(fit.poly, pars = c("alpha[651]", "alpha[582]", "alpha[420]", "alpha[177]"))
```

```{r}
#| label: fig-betaTrace
#| fig-cap: "Beta Traceplots"
traceplot(fit.poly, pars = c("beta[651]", "beta[582]", "beta[420]", "beta[177]"))
```

```{r}
#| label: fig-etaTrace
#| fig-cap: "Eta Traceplots"
traceplot(fit.poly, pars = c("eta[651]", "eta[582]", "eta[420]", "eta[177]"))
```
