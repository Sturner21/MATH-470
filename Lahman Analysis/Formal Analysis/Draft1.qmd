---
title: "Bayesian Data Analysis, Rough Draft"
author: "Sam Turner"
date: today
format: 
  html:
    toc: true
    toc-location: left
    toc-expand: true
    embed-resources: true
    
execute: 
  echo: false
  warning: false
---

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(rstan)
library(Lahman)
library(bayesplot)
library(tidybayes)
library(corrplot)
library(boot)
```

```{r}
#Set Stan to read the amount of cores that your computer can handel
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r}
#| message: false

#Creates a DF called "Batting" which has players stats from 1973-2019

#Load the DF's from Lahman directly
Batting <- Lahman::Batting
Teams <- Lahman::Teams
People <- Lahman::People

Batting <- Batting %>% 
  #Minimum year = DH was introduced; maximum year = before COVID
  filter(yearID >= 1973 & yearID <= 2019)

#Filter teams for years considered and select only necessary columns
Teams <- Teams %>% 
  filter(yearID>=1973 & yearID <= 2019) %>% 
  select(yearID, teamID, park)

#Combine current "Batting" and "Teams" DF to associate player with stadium played at
Batting <- Batting %>% 
  right_join(Teams, by = join_by(yearID, teamID))

#Add player's age to the "Batting" DF
Batting <- Batting %>% 
  right_join(Lahman::People, by = join_by(playerID)) %>% 
  mutate(Age = yearID - birthYear, .after=yearID) %>%
  #Don't consider the people who don't have a birthday in the database
  drop_na(Age)

#Make the lgID variable into an index variable
Batting <- Batting %>% 
  mutate(lg = case_when(lgID == "NL" ~ 1,
                        lgID == "AL" ~ 2), .after=lgID)


#Make an index for the parks to add them to the model

#The index goes from 1-88. No idea how it came up with them but I kept the park name in Batting so anyone should be able to reference to see how the parks impact player performance

Batting <- Batting %>% 
  mutate(park = as.factor(park)) %>% 
  mutate(park_index = as.integer(park), .after = park)

#Find qualified players who have over 50 AB's in each of the minimum 6 seasons played from 1973-2019
qualified_players <- Batting %>% group_by(playerID, Age) %>% 
  summarize(AB = sum(AB),
            HR = sum(HR)) %>% 
  filter(min(AB) >= 50, n() >= 6)

#Filter Batting DF for qualified players
Batting <- Batting %>% 
  filter(playerID %in% qualified_players$playerID)

#Calculated each players HR proportion "p"
Batting <- Batting %>% 
  mutate(p = HR / AB, .after=AB)
#You can see this for yourself too, but there are 657 players in this dataframe now

#Create an index for each player
Batting <- Batting %>% 
  mutate(player_index = as.factor(playerID), .after = playerID) %>% 
  mutate(player_index = as.integer(player_index))

#Create a centered age variable by subtracting 30 from everyone's age and square it for non-liner effect

Batting <- Batting %>% 
  mutate(cAge = Age -30, .after = Age) %>% 
  mutate(cAgeSq = cAge**2, .after = cAge)

#Make a year index because it's easier than having to create one everytime

Batting <- Batting %>% 
  mutate(year_index = yearID -1972, .after = yearID)
```

```{r}
#Make the data needed for the correlation plot
Batting_cor <- Batting %>%
  mutate(PA = AB + BB + IBB + HBP + SH + SF) %>% 
  select(AB, HR, PA, p, park_index, yearID, cAge) %>%
  filter(!is.na(p))
```

```{r}
#| eval: false
#Choose 500 players at random to consider
#If this doesn't take too long, then we can come back later and change this to possibly consider all players. Or just widen the initial time-frame considered and do 1000 players

set.seed(500)

N_player <- Batting %>% 
  distinct(playerID) %>% 
  pull(playerID) %>% 
  sample(size=500, replace=FALSE)
```

```{r}
#| eval: false
#Make a dataframe that only keeps the 500 players
Samp_Batting <- Batting %>% 
  filter(playerID %in% N_player)

head(Samp_Batting)
#If curious, this seed and selection covers all years. So, it should be able to handle the year-to-year effects
```

# Introduction and Motivations {#sec-intro}

To apply the methods discussed in *Statistical Rethinking* by Richard McElreath, Dr. Parson and I decided to replicate *Predicting Home Run Production in Major League Baseball Using a Bayesian Semiparametric Model* by Gilbert Fellingham and Jared Fisher. This article in particular has drawn our attention because of some unique and interesting modeling choices made by the authors. Both Dr. Parson and I believe that the initial investigation by Fellingham and Fisher (2017) is interesting, but there are some questionable modeling choices made by the authors throughout the paper. For example, there are few predictor variables and their associated priors are very weak. The paper does a good job with displaying plenty of visualizations for the reader, yet, under careful examination one is left with the impression that the authors have chosen to display a non-random assortment of players. This leaves careful readers wondering if the validity of their model is constant across all players, or if they have chosen a certain subset because these special players justify the use of their model. These reasons in tandem bring Fellingham and Fisher's (2017) findings into question. Thus, Dr. Parson and I have embarked to investigate the same research question as them: can we use a multilevel Bayesian model to predict a player's home run performance?

## Baseball Background {#sec-BaseBack}

In Major League Baseball (MLB) the team on offense is allowed to send one player, at a time, up to home plate to "bat." Any time a player is sent up to bat this constitutes a plate appearance (PA). Depending on the outcome of the PA it may or may not be considered an at-bat (AB). If the PA ends in the player being walked (BB \[Base on Balls\]) then it is not recorded as an AB because it is thought the player did not receive a fair chance at hitting the baseball. On the other hand, if the PA ends in the player getting a hit or getting out, then it is assumed that he had a fair chance at hitting the ball and the PA is recorded as an AB. This distinction to the average fan is mostly arbitrary. However, in a modeling context it is very important. When modeling a player's ability to hit home runs (HR), as this paper is concerned with, the distinction is important because it is unfair to consider all the PA's of a player because this set of outcomes includes outcomes where it is assumed that the player has not been given a fair chance to hit the ball. For similar reasons, considering the subset of outcomes where it is assumed that they player has been given a fair chance to hit the ball seems more appropriate, this subset is AB's. Thus, this paper uses AB's as the main predictor for home runs hit by a certain player. Fellingham and Fisher (2017) also agree with this reasoning and use AB's, rather than PA's, in their research.

Furthermore, it is logical to ask: why not consider both? Incorporating more predictors can make a model more accurate at prediction, which both Fellingham and Fisher (2017) and this paper are concerned with. The reasons this paper does not consider both is because, especially in a Bayesian context, researchers must be concerned with multicollinearity. Without a doubt there is a strong correlation between the two because one is a subset of the other. In fact, we can see that the correlation between the two is quite strong in @fig-BattingCor. When multicollinearity is introduced into a model, it can be difficult for the Markov Chain Monte Carlo (MCMC) algorithm to parse out which of the two variables is responsible for the changes in the dependent variable. Thus, we neglect PA's from our model both because theory stipulates it is an inferior predictor and used in tandem with our current predictor it would cause problems for the modeling software.

An interesting quirk of baseball is that, like soccer, stadiums differ in dimensions. This is not universal; baseball fields and stadiums must have the same measurements for their infields. Yet, for the most part baseball stadiums are allowed to configure their outfields as they please. This becomes important because most HR's are achieved by hitting the baseball over the outfield fence. Thus, fences closer to home plate generally lead to more HR's for all hitters, ceteris paribus. The non-standardized nature of stadiums' fences gives them a variety of unique shapes and ranging outfield sizes. For example, Fenway Park (1923-) is known to have the closest outfield wall with their "Green Monster" in left field being only 305 ft. from home plate. While the Polo Grounds (1891-1963) boasted one of the furthest walls from home plate at 483 ft. in center field. Stadiums also differ in location. Marlins Park (2012-2020) sits at 15 ft. above sea level while Coors Field (1995-) sits at 5183 ft. above sea level. As illustrated there are not only vastly different dimensions that baseball fields can take on but also different environments which affect a player's ability to hit home runs. Thus, any model which attempts to model players' ability to hit HR's needs to consider ballpark factors.

MLB, as we know it, has been played since about 1871 and the origins of the game go back even father than that. Over this time the some rules of the game have undergone incredible transformations which vastly change the nature of the game. For example, from 1900-1920 baseball was in the "deadball era." During this period the materials used to create baseballs made it so that the ball absorbed much more force than it does today. This had a plethora of implications for the game and how it was played; HR's were much less common because they were harder to hit. In contrast, the period from 1994-2004 is known informally as the "steroid era" because of the rampant use of performance enhancing drugs (PED) used by players. This, in combination with other factors such as harder materials used to make baseballs, led to players obliterating a variety of previous hitting records, such as the career and season home runs records. It is obvious from these facts that decade played for player's had a drastic impact on their ability to produce outcomes. In fact, this can be visualized by considering all the runs scored during a given season from 1871-2019 displayed in @fig-RunsByYear.

Finally, age is an important factor in athletic ability. To an extent in MLB, seniority can act both as a blessing and a curse. On the one hand, older players are less able to hit home runs because their body is starting to lose its peak athletic performance it had when it was, say, 30. On the other hand, these older players also have more experience than younger players and thus may be better able to hit home runs from strategies they have found during their time playing. Further we can visualize this by looking at the median home runs hit by each player from 1871-2022, which is displayed in @fig-HR.By.Age.All. However, this graph does not depict some of the nuances of the data well. For example, we have almost no data on how often 18 -year-olds hit HR's because they play in MLB infrequently. As can be seen with the standard error bars on the ages above 40, older players suffer a similar problem of small sample size. Instead, consider @fig-HR.By.Age.20to40 which shows only considers players who are 20-40 years old during the season. This graph clearly shows the complex non-linear relationship between player performance and age. It shows that players are generally improving in their ability to hit HR's from 20-30 years old. Then, however, we see a slight downwards trend followed by an upward trend with increasing error bars. This shows that as players age past 30 their abilities start to decline. However, the jump afterwards can probably be attributed to players who are very good, continuing their careers. If a player is very good then they are going to be able to perform at above average when they are, say, 35. However, if the player is not generally considered good, then a team will likely opt to try out a new young rookie who may not be as good in the moment but is more projectable. Thus, we see less players playing past 35 which explains the higher standard errors, but they are of a higher caliber than any other age group which is why they are still playing, thus we see their proportion as higher.

## Literature Review {#sec-LitReview}

As mentioned, Fellingham and Fisher (2017) have both considered and published their thoughts on this research question. This research question concerns predicting player home run performance using Bayesian multilevel modeling. To do this, Fellingham and Fisher (2017) used data from the Lahman data set spanning 1871 to 2008 to estimate the parametric weights for their model. Over this time frame they only considered players who had played for at least 6 seasons and who, in each of those seasons, had at least 50 AB's. They only considered seasons where the players were 18-years-old to 45-years-old.

To predict a player's HR's they used a binomial logistic model. This model predicts HR's using the AB's of a player for the $n$ trials and the underlying ability of a player to hit a HR, $\pi$ as their probability of success. Fellingham and Fisher (2017) estimate a player's ability to hit home runs from his age in a season, decade born, season played, and home ball park. This leaves Fellingham and Fisher with the following model, which we have adapted to make model parameters the same across both their model and my model (in @sec-Model):

$$
\begin{align}
HR_{nip}|\pi_{nip}&\sim Binomial(AB_{nip},\pi_{nip})\\
\\
\log(\frac{\pi_{nip}}{1-\pi_{nip}})&=[\beta_{n1}\cdot Age_{ni}]+[\beta_{n2}\cdot(Age_{ni})^2]+[\beta_{n3}\cdot(Age_{ni})^3]+[\beta_{n4}\cdot(Age_{ni})^4]+\alpha_n+\delta_i
+\xi_p
\end{align}
$$

Where $HR_{nip}$ is the HR's hit by a player $n$ in year $i$ played at stadium $p$. $AB_{nip}$ is the AB's for player $n$ in year $i$ at stadium $p$. $\pi_{nip}$ is the probability of a HR in an AB for player $n$ in year $i$ at stadium $p$.

The parameter's $\beta_{n,1-4}$ describe the weights of the orthogonal polynomial for, up to the quartic, ages of players. We theorize, because Fellingham and Fisher (2017) did not explicitly articulate why, that they chose a quartic model because it has a closer fit to the data displayed in @fig-HR.By.Age.All and @fig-HR.By.Age.20to40. The properties of a squared term is that it captures the rise and fall of player's HR probability. However, the inclusion of the cubic and quartic term allow for for modeling two humps, or peaks, in a data set. This appears in @fig-HR.By.Age.20to40 where players see a peak at 27 and then their probability decreases a little, which is then followed by another peak at 31, and afterwards continues in a downward trend. This will be discussed in more depth in @sec-Model, but our model which aims to improve Fellingham and Fisher's (2017) model does not account for the cubic or quartic effects over fear of over-fitting the age data.

The parameter $\alpha_n$ (which is $\theta_i$ in the actual paper) are the individual intercepts for each player. This parameter is the only one that Fellingham and Fisher (2017) use which takes advantage of multilevel modeling. In this modeling, Fellingham and Fisher (2017) set the mean of $\alpha_n$ to be $\mu_d$ where $d$ is the decade the player was born. That means that Fellingham and Fisher (2017) set the parameter which measures the player's "innate" HR probability based on the HR probability of other players, who were born in the same decade. This allows Fellingham and Fisher (2017) to fit make confident predictions about players who may have only played for 6 seasons because they know that their model is trained on more than 6 data points. This parameter, $\alpha_n$ along with $\beta_{n,1-4}$ represent the player specific terms of the model. That is, the parameters of the model which depend solely on the player being considered.

In contrast, the parameter $\delta_i$ represents the effect that season $i$ has on player $n$'s ability to hit a HR at stadium $p$. This $\delta$ is calculated for each season considered where $i\in\{1,…,138\}$ for the 138 seasons considered. This $\delta_i$ is calculated independent of each player so it represents the different changes that we see in HR ability over all years considered. An example of why this effect is important refer to @fig-HRByYear.

The parameter $\xi_p$ is the effect that playing in park $p$ has on the HR probability $\pi_{nip}$ in year $i$. Fellingham and Fisher (2017) decided that they want this estimate to independent of the hitter. Hence, they use no pooling or multilevel effects in the model. They estimate the effect that each park has on a player's ability to hit a HR and then add the associated effect to the players who play at that ballpark.

What makes Fellingham and Fisher's model so strange are the priors that Fellingham and Fisher (2017) chose. Disregarding that there is no justification for these priors, Fellingham and Fisher (2017) set the following priors for these parameters:

$$
\begin{align}
\alpha_n&\sim Normal(\mu_{d_n},\sigma^2_{d_n}), d\in \{1...14\}, n\in\{1,...,3735\}\\
\\
\mu_d&\sim Normal(-2.5,1), d\in\{1,...,14\}\\
\\
\sigma^2_{d}&\sim Gamma(3,3), d\in\{1,...14\} \\
\\
\delta_i&\sim Normal(-2.5, 2),i\in \{1,...,138\}\\
\\
\xi_p&\sim Normal(-2.5, 2), p\in \{1,...,222\}\\
\end{align}
$$

These priors are quite strange because they are so broad. For example, consider the priors on either $\delta_j$ or $\xi_p$. The priors state that on a logistic scale, it is normal for the decade of birth and season of play parameters to vary along the interval \[-6.5, 1.5\]. On the normal probability scale, this corresponds to probability fluctuations of about (0.0015, 0.81). Furthermore, if you only consider the means that Fellingham and Fisher have used here, then you would be left with a player who, on the logit scale, hits HR's at a probability of -7.5. On the normal probability scale this becomes about 0.00055, which is 0.055% chance any AB ends in a HR. Confusingly, this is not even close to the 2.8% that Fellingham and Fisher (2017) even noted in their paper. Surely there must be better priors than this which Fellingham and Fisher (2017) could have used since it does not seem that even they believe these priors that they have set. Coincidentally, this is probably why in using MCMC to estimate their parameters they needed to use 200,000 iterations. Likely, their model could have estimated their parameters much more easily had they given stronger priors. This is one of the areas this paper seeks to improve Fellingham and Fisher's (2017) methods.

Another questionable choice that Fellingham and Fisher (2017) make is that they estimated their model parameters using FORTRAN. Given the antiquity of FORTRAN, replication of Fellingham and Fisher (2017) is quite difficult. Not to mention that the 200,000 iterations makes the research so computationally expensive that it makes it nearly impossible for any other research team to replicate. This is disappointing because the scientific method relies on replication for the advancement of knowledge. Thus, our research seeks to make the algorithms used in this research more accessible because we chose to use R to interface with the modeling software Stan, which is written in the (more contemporary) C++ programming language.

Finally, we see that Fellingham and Fisher (2017) project the performance of a number of players from their considered 3,735. However, upon closer inspection these players do not seem to be randomly selected because the selected players were pretty renown during their prime. Nor, does Fellingham and Fisher (2017) make the assurance that these players are randomly selected. This should leave readers with unease because it is unclear how well their model fits most players, which seemed to be what the paper was trying to investigate. Although unlikely, it is conceivable that they only selected players which make their model appear the most reliable. Instead, our paper seeks to remedy this by considering a random assortment of players to examine the fit of our model.

# Model {#sec-Model}

Given all the previous problems with Fellingham and Fisher (2017) mentioned above, there is an argument to be made that a simple generalized linear model (GLM) could be used instead of their complicated Bayesian semiparametric model. While it is true that a GLM could be used to predict HR's of a player it would not nearly be as accurate as a Bayesian model. Nor would it be able to as accurately capture the non-linearities present in the data. This is because the greatest strength of using the Bayesian technique is that it allows for multilevel models to be created. That is, Bayesian models allow for models to "learn" from other players and group certain players when there is not as much data available for them. In turn, this leads to better predictions about players where data is lacking. This "learning" is known as partial pooling, where the model identifies that parameters for each player (or year etc.) probably come from the same distribution but may not share the exact same estimates.

Exploring concretely how our model works will make it clearer than discussing it in the abstract. The current model that we have is the following:

$$
\begin{align}
HR_{nip} &\sim Binomial(AB_{nip},\pi_{nip})\\
\log(\frac{\pi_{nip}}{1-\pi_{nip}}) &= \alpha_{n}+\beta_n\cdot (Age_{ni}-30)+\eta_n\cdot (Age_{ni}-30)^2+\delta_p+\xi_i
\end{align}
$$

$$
\begin{align}
\alpha_n&\sim Normal(\mu_0, \sigma_0)\\
\beta_h&\sim Normal(\mu_1, \sigma_1)\\
\eta_h&\sim Normal(\mu_2, \sigma_2)\\
\delta_p&\sim Normal(\mu_5, \sigma_5)\\
\xi_i&\sim Normal(\mu_6, \sigma_6)\\
\end{align}
$$

$$
\begin{align}
\mu_0 &\sim Normal(-3.5, 0.1)\\
\sigma_0&\sim Exponential(1)\\
\\
\mu_1&\sim Normal(0,0.01)\\
\sigma_1&\sim Exponential(10)\\
\\
\mu_2&\sim Normal(0,0.01)\\
\sigma_2&\sim Exponential(100)\\
\\
\mu_5&\sim Normal(0,0.01)\\
\sigma_5&\sim Exponential(10)\\
\\
\mu_6&\sim Normal(0,0.25)\\
\sigma_6&\sim Exponential(10)
\end{align}
$$

In this model, we say that the $HR$'s of player $n$ follow a binomial distribution with number of trials for player $n$ in season $i$ are $AB_{ni}$. The probability of success (hitting a HR) denoted as $\pi_{ni}$ for player $n$ in season $i$. Furthermore, we measure $\pi$ on the logistic scale.

As discussed in @sec-BaseBack, there are many factors that influence player $n$'s underlying probability of hitting a $HR$ in a given $AB$ during season $i$. The first parameter which affects $\pi$ is $\alpha_n$. In our model $\alpha_n$ can be generally thought of as the term which is used to account for most player's innate ability to hit HR's. This comes from our exploratory observations of the data. In @fig-HR.By.Age.20to40, @fig-pDist, and @fit-pDistSmall, we noticed that the average (median or mean) proportion of AB's which resulted in HR's to be about 0.03. This means that about 3 in every 100 AB's result in HR's. On the logit scale, which is what our model relies on, this is about -3.5. However, we recognize this depends on the hitter which is where multilevel modeling becomes useful. Instead of simply using the average probability as our intercept, which is what a dummy variable would do in a GLM, we are able to allow our model choose an intercept which agrees with the data but can adapt depending on the player. Thus, we let $\alpha$ be normally distributed with mean $\mu_0$ (where $\mu_0$ is normally distributed with mean -3.5 and standard deviation 0.1) and standard deviation $\sigma_0$ (where the parameter $\sigma_0$ itself is exponentially distributed). By allowing $\mu_0$ and $\sigma_0$ to have their own distributions, we are able to have the distribution of $\alpha_n$ adapt to the player's specific underlying probability of hitting HR's. That is, we are able to consider what can be most closely considered their innate ability because this estimate for $\alpha_n$ does not account for other considered factors, such as age, park factors, or year played.

The next parameter we considered which affects $\pi_{ni}$ was $\beta_h$ which represents the effect age $h$, on the centered scale, has on a player's ability to hit HR's. This topic was briefly touched upon in @sec-BaseBack, where we discussed how player age can impact their their strategy and physical prowess. These ideas are well demonstrated in @fig-HR.By.Age.20to40. In @fig-HR.By.Age.20to40, one can easily see that, in general, players ability to hit HR's improves over the course of their career. As a note, we centered the the age of the all players at 30 because not only was that roughly the mean and the median, but also because it makes interpreting the $\beta_h$ easier. The value of $\beta_h$ represents how changes away from the centered age affect player $n$'s probability of hitting a HR, $\pi_{ni}$. In normal GLM's, $\beta_h$ would be general to age, meaning that there would be one intercept for each age that fit all players in the data the best. However, as with $\alpha_n$, we have employed a multilevel model to allow for each $\beta_h$ to be specific to each player. It becomes more difficult to interpret the values of $\beta_h$ because they correspond to the multiplicative effect of the centered age index. That is, in calculating the value of $\beta_h$, our model considers the priors we have set with $\mu_1$ and $\sigma_1$ to create a distribution for $\beta_h$ and then multiplies this $\beta$ with our centered age value $h$. In essence, this means that our $\beta_h$ describes the multiplicative effect of being a certain distance from the centered age, and how that effects player's $\pi$. Another important note, is that although this effect of $\beta$ is multiplicative with respect to the player's index, $h$, it is additive with respect to $\alpha$ and therefore $\pi$. This means that the whatever the model predicts for $\beta_h$ and its weight, it must be added to $\alpha_n$ to generate the correct $\pi$.

Similarly $\eta_h$ is the effect that a player's age squared has on the probability of a HR in an AB. The purpose of this squared term is to capture the non-linearity of HR as player's age. It is apparent in both @fig-HR.By.Age.20to40 and @fig-HR.By.Age.All that there is a non-linear effect of age on a player's ability to hit HR's. This ability appears to peak about when the player is 30-years-old. We include $\eta_h$ in the model to capture this downward trend that we see after a player reaches 30-years-old. It is nearly impossible to interpret what $\eta_h$ is doing because it is the multiplicative effect of the player's centered age squared. This begs the question, what even is a squared age? How does one interpret it? There is not a clear answer on this question, so it is best understood that this term simply accounts for the non-linearity we see in the data and not become too concerned with what the actual estimates are. However, it is easy to justify our choice for the priors on $\eta_h$, $\mu_2$ and $\sigma_2$. We chose $\mu_2\sim Normal(0,0.01)$ because we assumed that there would be a negative effect so small that it would be very close to zero. Further, any distinction to the slightly negative of zero would be hard to justify and, in essence, arbitrary. Therefore, we went with the mean of 0 and the standard deviation of 0.01 to account for any players who have a more or less extreme non-linearity to their trajectories. That is, higher or lower peaks and troughs throughout their career for their probability $\pi$. We chose $\sigma_2\sim Exponential(100)$ because standard deviations cannot be negative, which the exponential distribution lends itself nicely to. Furthermore, the exponential distribution with a parameter of 100 is very concentrated. Since we know that the coefficient of $\eta_h$ is likely to be very small since it is being multiplied by the age squared (which can be up over 100). Thus, we can be confident that our standard deviation will end up being small for our estimate.

Another parameter to consider the the effect that playing in certain ballparks has on HR probability $\pi$. This was discussed in @sec-BaseBack, where we highlighted that players in certain ballparks are more likely to hit HR's than in others because of the different dimensions of the outfield fences. Furthermore, we know that these parks will effect players on different teams differently because of how schedules are created. Players will play half of their games at their home-stadium and the other half at various other ballparks known as "on the road." Thus, the home stadium that player plays at is an important factor for their HR probability $\pi$. In our model, we have estimated the effect that each ballpark has on player's playing in that ballpark. Stadiums like Great American Ballpark (index=33) tend to have a better HR effect than stadiums such as Cleveland Stadium (index=19). We account for this difference in our model with the parameter $\delta_p$ where $\delta$ is the effect that park $p$ has on a player's HR probability $\pi$. Interpreting $\delta_p$ is much easier than the previous two parameters. It simply states, on the logit scale, how the park effects the underlying probability of hitting a HR, $\pi$. Similar to $\alpha_n$ this effect is additive and can act almost like an intercept in the model. It also takes advantage of multilevel modeling since we have set priors which can adapt to the data on it, $\mu_5$ and $\sigma_5$. We have purposely set the parameter for $\mu_5\sim Normal(0,0.01)$ with mean 0 because we assume that parks in the aggregate will not have an effect on a player's ability to hit HR's. That is, for every Cleveland Stadium there exists a Great American Ballpark to counteract this. There is no formal justification for this, it is just an assumption that we make based on our preliminary exploration and our understanding that ownership (who controls the dimensions of the stadium) are not particularly biased to any ballpark size which means that we should see good variation between parks. Further, we assume that a fair standard deviation for $\mu_5$ is 0.01 because on the logistic scale, this means that we could see about a two percent change in the probabilities between any given stadiums. At most we would see a 4% difference. We chose a standard deviation for $\delta_p$ of $\sigma_5\sim Exponential(10)$ for similar reasons to all of the other $\sigma$'s. The exponential distribution cannot be negative, which we need for standard deviations. Further, we know that the effect of each park will probably not be huge because, although there is deviation, there is not extreme deviation between parks that makes some "unfair." Thus, with our parameter of 10 for the exponential distribution we are being fairly confident with our model about our estimates of the priors.

Our final parameter, $\xi_i$ represents the change in the underlying probability of player hitting a HR in the year $i$. This stems mostly from the observation in @fig-HRByYear which shows a steep increase in the amount of HR's hit per year across any subset of years considered. This means that there is an underlying change in the $\pi$ depending on the year considered. The reason for this change is irrelevant. It could be because batters are becoming better or pitchers are getting worse, or some combination of the two. Regardless, we can see that there is an increase and we need to account for this in our estimates for $\pi$ of player $n$. Thus, we include $\xi_i$ to represent the effect that playing in a certain year will have on the player's ability to hit a HR. We set the mean of $\xi_i$ as $\mu_6\sim Normal(0, 0.25)$ because we can see that the HR proportion change year over year in @fig-pByYear by only fractions of a percent. Thus, when we set our prior with mean 0 and standard deviation of 0.25, we are saying that we expect most year of year variation to be from \[-4, -3\] on the logit scale. On the normal this is \[0.018, 0.047\] which means that we would expect at most a change of 3% year over year. This is much larger than we would actually anticipate in a change but it allows the model to learn from the data to learn from the data more since we are unsure of exactly what the change beyond knowing that there is a change year over year.

EXPLORE THE DATA HERE AND WHY A MULTILEVEL MODEL WORKS BEST

CHOOSE A FEW PLAYERS TO SHOW HOW THE MODEL FIT CHANGES FOR EACH PLAYER

# Results {#sec-Results}

```{r}
#| eval: false

#Since we aren't fitting anything when we render, we don't need this chunk to take up any extra time calculating usless vectors
data.poly.fit <- list(
  N = nrow(Batting),
  n_player = n_distinct(Batting$player_index),
  n_park = n_distinct(Batting$park_index),
  n_year = n_distinct(Batting$yearID),
  AB = Batting %>% pull(AB),
  HR = Batting %>% pull(HR),
  Player = Batting %>% pull(player_index),
  cAge = Batting %>% pull(cAge),
  cAgeSq = (Batting %>% pull(cAge))**2,
  Park = Batting %>% pull(park_index),
  Year = Batting %>% pull(yearID) -1972
)
```

```{r}
#| eval: false

#We don't want the program, which takes my desktop 30-45 minutes to render, to run every time I need to render the docuement
fit.poly <- stan(
  file = "HRPolyFit.stan",
  data = data.poly.fit,
  iter = 1e4
)
```

```{r}
#| eval: false

#This saves the fit as and RDS so that I can reference it later when rendering
saveRDS(fit.poly, "PolyFit.rds")
```

```{r}
#Lets me load in the model that I made. Will only work on my desktop.
fit.poly <- readRDS("C:/Users/Sam/Desktop/RDS/PolyFit.rds", )
```

```{r}
#| eval: false
plot(fit.poly, pars="delta")
```

```{r}
#| eval: false
print(fit.poly, pars="delta")
```

# Figures {#sec-Figures}

```{r}
#| label: fig-BattingCor
corrplot(cor(Batting_cor), type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

```{r}
#Make the dataframe needed for Runs Scored by year graph
Runs.By.Year <- Lahman::Teams %>% 
  group_by(yearID) %>% 
  summarize(R = sum(R))
```

```{r}
#| label: fig-RunsByYear
#| fig-cap: "Cumulative Runs Scored in MLB by Year"

ggplot(data = Runs.By.Year)+
  aes(x=yearID, y=R)+
  geom_line()+
  geom_rect(inherit.aes = FALSE, aes(xmin=1900, xmax=1920, ymin=0, ymax=26000), color="transparent", fill="red", alpha=0.005)+
  geom_rect(inherit.aes = FALSE, aes(xmin=1994, xmax=2004, ymin=0, ymax=26000), color="transparent", fill="orange", alpha=0.005)+
  scale_y_continuous(limits =c(0,26000), breaks = seq(from=0, to=25000, by=5000))+
  scale_x_continuous(breaks = seq(from=1870, to=2020, by=10))+
  labs(title = "Cumulative Runs Scored in MLB by Year",
       x = "Year",
       y = "Runs Scored")
```

```{r}
HomeRuns.By.Year <- Lahman::Teams %>% 
  group_by(yearID) %>% 
  summarize(HR = sum(HR),
            AB = sum(AB)) %>% 
  mutate(p = HR / AB)
```

```{r}
#| label: fig-HRByYear
#| fig-cap: "Cumulative Home Runs in MLB by Year"
ggplot(data = HomeRuns.By.Year)+
  aes(x=yearID, y=HR)+
  geom_line()+
  scale_x_continuous(n.breaks = 20)+
  labs(title = "Cumulative Home Runs in MLB by Year",
       x = "Year",
       y = "Home Runs - League Cumulative")
```

```{r}
#| label: fig-pByYear
#| fig-cap: "Cumulative Home Run Probability in MLB by Year"
ggplot(data = HomeRuns.By.Year)+
  aes(x=yearID, y=p)+
  geom_line()+
  scale_x_continuous(n.breaks = 20)+
  labs(title = "Cumulative Home Run Probability in MLB by Year",
       x = "Year",
       y = "HR Proportion (HR/AB) - League Cumulative")
```

```{r}
#Make DF which describes the HR by age across the timeline considered
HR.Prop.Age <- Batting %>% 
  group_by(Age) %>% 
  summarize(HR = sum(HR), AB = sum(AB)) %>% 
  mutate(prop = HR / AB, prop.se = sqrt(prop*(1-prop)/AB))
```

```{r}
#| label: fig-HR.By.Age.All
#| fig-cap: "Proportion of Home Runs per At-Bat by Age"

ggplot(data = HR.Prop.Age)+
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - prop.se,
                                ymax = prop + prop.se))+
  labs(title = "Proportion of Home Runs per At-Bat by Age",
       x = "Age",
       y = "Proportion of HR per AB")
```

```{r}
#| label: fig-HR.By.Age.20to40
#| fig-cap: "Proportion of Home Runs per At-Bat by Age"

ggplot(data = HR.Prop.Age %>% filter(Age >=20 & Age <= 40))+
  geom_pointrange(mapping = aes(x = Age, y = prop, ymin = prop - prop.se,
                                ymax = prop + prop.se))+
  labs(title = "Proportion of Home Runs per At-Bat by Age",
       x = "Age",
       y = "Proportion of HR per AB")
```

```{r}
#| label: fig-pDist
ggplot(data = Batting)+
  aes(x = p)+
  geom_histogram(bins = 75)+
  geom_vline(xintercept=median(Batting$p, na.rm = TRUE) , color="red")+
  scale_x_continuous(n.breaks = 10)+
  labs(title = "Distribution of Players' HR Proportions",
       subtitle = "Seasons Considered: 1973-2019",
       x = "Proportions (p)",
       y = "Observations")
```

```{r}
#| label: fig-pDistSmall
#I want this to have a little more breaks in the x-axis to highlight where the median is
ggplot(data = Batting)+
  aes(x = p)+
  geom_histogram(bins = 75)+
  geom_vline(xintercept=median(Batting$p, na.rm = TRUE) , color="red")+
  scale_x_continuous(n.breaks = 16, limits = c(0,0.15))+
  labs(title = "Constricted View of Distribution of Players' HR Proportions",
       subtitle = "Seasons Considered: 1973-2019",
       x = "Proportions (p)",
       y = "Observations")
```

```{r}
#The selected player (id=651) is Robin Yount

set.seed(1)

select_player <- Batting %>% 
  distinct(playerID) %>% 
  pull(playerID) %>% 
  sample(size=1, replace=FALSE)

select_player
```

```{r}
select_player_Batting <- Batting %>% 
  filter(playerID %in% select_player) %>% 
  select(playerID, player_index, yearID, year_index, Age, cAge, cAgeSq, AB, HR, p, park_index)

select_player_Batting
```

```{r}
#These first three effects (alpha, beta, and eta) are all corresponding to the 651st player (Robin Yount)
alpha_i651 <- fit.poly %>% 
  spread_draws(alpha[group]) %>% 
  summarise_draws() %>% 
  filter(group==651)

beta_i651 <- fit.poly %>% 
  spread_draws(beta[group]) %>% 
  summarise_draws() %>% 
  filter(group==651)

eta_i651 <- fit.poly %>% 
  spread_draws(eta[group]) %>% 
  summarise_draws() %>% 
  filter(group==651)

#Meanwhile, this value corresponds to the park
delta_i651 <- fit.poly %>% 
  spread_draws(delta[group]) %>% 
  summarise_draws() %>% 
  filter(group==24)

#And this value corresponds to the years that he played
xi_i651 <- fit.poly %>% 
  spread_draws(xi[group]) %>% 
  summarise_draws() %>% 
  filter(group >= 2 & group <= 21)
```

```{r}
#This outputs the associated means for each index for player 651
alpha_i651 %>% pull(mean)
beta_i651 %>% pull(mean)
eta_i651 %>% pull(mean)
delta_i651 %>% pull(mean)
xi_i651 %>% pull(mean)
```

```{r}
select_player_Batting <- select_player_Batting %>% 
  mutate(alpha = alpha_i651 %>% pull(mean), .after = player_index) %>%
  mutate(beta = beta_i651 %>% pull(mean), .after = cAge) %>% 
  mutate(eta = eta_i651 %>% pull(mean), .after = cAgeSq) %>% 
  mutate(delta = delta_i651 %>% pull(mean), .after = park_index) %>% 
  mutate(xi = xi_i651 %>% pull(mean), .after = year_index) %>% 
  mutate(pred = alpha + (beta*cAge) + (eta*cAgeSq) + (delta) + (xi) , .after = p) %>% 
  mutate(inv_logit_pred = inv.logit(pred), .after=pred)
```

```{r}
ggplot(data = select_player_Batting)+
  aes(x = cAge, y = p)+
  geom_point()+
  geom_smooth()+
  labs(title = "Actual Trajectory for player #651")
```

```{r}
ggplot(data = select_player_Batting)+
  aes(x = cAge, y=inv_logit_pred)+
  geom_point()+
  geom_smooth()+
  labs(title = "Predicted Career Trajectory for #651")
```
