---
title: "Practice Problems"
format: html
---

# Load Libraries

```{r}
library(rethinking)
library(tidyverse)
```

# 7E1

State the three motivating criteria that define information entropy. Try to express each in your own words.

1.  Uncertainty measures should be continuous so that arbitrarily small changes in probabilities do not correspond to massive shifts in uncertainty.
2.  The measure of uncertainty should increase as the number of possible events increases.
3.  The measure of uncertainty should be additive.

# 7E2

Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

```{r}
p_coin <- c(0.7, 0.3)
-sum(p_coin * log(p_coin))
```

# 7E3

Suppose a four-sided die is loaded such that, when tossed onto a table, it shows "1" 20%, "2" 25%, "3" 25%, and "4" 30% of the time. What is the entropy of this die?

```{r}
p_die1 <- c(0.2, 0.25, 0.25, 0.3)
-sum(p_die1 * log(p_die1))
```

# 7E4

Suppose another four-sided die is loaded such that it never shows "4". The other three sides show equally often. What is the entropy for of this die?

```{r}
p_die2 <- c(1/3, 1/3, 1/3)
-sum(p_die2 * log(p_die2))
```

# 7M1

Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criteria into a less general one?

# 7M2

Explain the differences between model selection and model comparison. What information is lost under model selection.

Model selection is where you choose a model which has the lowest specified criterion value and then discarding the other models. While model comparison is where you keep all models but you weight what the models say based on their criterion value. You do not discard any of them.

Under model selection you lose the information about relative model accuracy contained between the different CV/PSIS/WAIC values. Sometimes this is large and sometimes this is small, but discarding the information prevents this step from being performed.

# 7M3\*

When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Provide some examples, if you are not sure.

# 7M4

What happens to the effective nymber of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

# 7M5

Provide an informal explanation of why informative priors reduce overfitting.

When priors are not informative, our models get really excited about the data and basically only have this data to go off of in estimation. Thus, when using small sample sizes where these observations may happen to be wrong it can lead to the model overfit to just the small number of observations provided. Giving informative priors means that the model will be more skeptical when being given observations and lead to it incorporating these observations more conservatively into its posterior and predictions.

# 7M6

Provide and informal explanation of why overly informative priors result in underfitting.

For a similar reasons above as to why informative priors reduce overfitting. Making priors too skepticals will mean that they ignore the data given to them which in turn will lead to the model being underfit. In other words the model just won't pay attention to the information given to it and will insist that its priors are the best.

# 7H1

# 7H2

# 7H3

# 7H4

# 7H5
