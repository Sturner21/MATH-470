---
title: "Practice Problems"
format: html
---

# Load Libraries

```{r}
library(rethinking)
library(tidyverse)
```

# 7E1

State the three motivating criteria that define information entropy. Try to express each in your own words.

1.  Uncertainty measures should be continuous so that arbitrarily small changes in probabilities do not correspond to massive shifts in uncertainty.
2.  The measure of uncertainty should increase as the number of possible events increases.
3.  The measure of uncertainty should be additive.

# 7E2

Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

```{r}
p_coin <- c(0.7, 0.3)
-sum(p_coin * log(p_coin))
```

# 7E3

Suppose a four-sided die is loaded such that, when tossed onto a table, it shows "1" 20%, "2" 25%, "3" 25%, and "4" 30% of the time. What is the entropy of this die?

```{r}
p_die1 <- c(0.2, 0.25, 0.25, 0.3)
-sum(p_die1 * log(p_die1))
```

# 7E4

Suppose another four-sided die is loaded such that it never shows "4". The other three sides show equally often. What is the entropy for of this die?

```{r}
p_die2 <- c(1/3, 1/3, 1/3)
-sum(p_die2 * log(p_die2))
```

# 7M1

Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criteria into a less general one?

$$
AIC=-2*LLPD+2p\\
WAIC(y,\Theta)=-2(LLPD-\sum var_{\theta}logp(y_i|\theta))
$$

Of these information criterion WAIC is more general. In order to transform WAIC into AIC you need the following conditions:

1.  The priors are flat or overwhelmed by the likelihood.
2.  The posterior distribution is approximately multivariate Gaussian.
3.  The sample size $N$ is much greater than the number of parameters $k$.

# 7M2

Explain the differences between model selection and model comparison. What information is lost under model selection.

Model selection is where you choose a model which has the lowest specified criterion value and then discarding the other models. While model comparison is where you keep all models but you weight what the models say based on their criterion value. You do not discard any of them.

Under model selection you lose the information about relative model accuracy contained between the different CV/PSIS/WAIC values. Sometimes this is large and sometimes this is small, but discarding the information prevents this step from being performed.

# 7M3\*

When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Provide some examples, if you are not sure.

Is this a trick question? It's obvious that if models are fit to different training data then they are different models entirely and not just variations on assumptions.

# 7M4\*

What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

Shouldn't it become smaller? Since as you get more data they will drown out the noise of overfitting created by the extra parameters.

```{r}
#Load his data on cars and create model with "quap"
data(cars)
m <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(0,100),
    b ~ dnorm(0,10),
    sigma ~ dexp(1)
  ) , data=cars )
set.seed(94)
post <- extract.samples(m,n=1000)
```

```{r}
n_samples <- 1000
logprob <- sapply( 1:n_samples ,
  function(s) {
    mu <- post$a[s] + post$b[s]*cars$speed
    dnorm( cars$dist , mu , post$sigma[s] , log=TRUE )
  } )

n_cases <- nrow(cars)
lppd <- sapply( 1:n_cases , function(i) log_sum_exp(logprob[i,]) - log(n_samples) )

pWAIC <- sapply( 1:n_cases , function(i) var(logprob[i,]) )

sum(pWAIC)
```

```{r}
lm(data=cars, dist ~ speed) %>% 
  summary()
```

```{r}
m <- quap(
  alist(
    dist ~ dnorm(mu,sigma),
    mu <- a + b*speed,
    a ~ dnorm(-17.6, 1),
    b ~ dnorm(3.9,0.1),
    sigma ~ dnorm(15,2)
  ) , data=cars )
set.seed(94)
post <- extract.samples(m,n=1000)

n_samples <- 1000
logprob <- sapply( 1:n_samples ,
  function(s) {
    mu <- post$a[s] + post$b[s]*cars$speed
    dnorm( cars$dist , mu , post$sigma[s] , log=TRUE )
  } )

n_cases <- nrow(cars)
lppd <- sapply( 1:n_cases , function(i) log_sum_exp(logprob[i,]) - log(n_samples) )

pWAIC <- sapply( 1:n_cases , function(i) var(logprob[i,]) )

sum(pWAIC)
```

# 7M5

Provide an informal explanation of why informative priors reduce overfitting.

When priors are not informative, our models get really excited about the data and basically only have this data to go off of in estimation. Thus, when using small sample sizes where these observations may happen to be wrong it can lead to the model overfit to just the small number of observations provided. Giving informative priors means that the model will be more skeptical when being given observations and lead to it incorporating these observations more conservatively into its posterior and predictions.

# 7M6

Provide and informal explanation of why overly informative priors result in underfitting.

For a similar reasons above as to why informative priors reduce overfitting. Making priors too skepticals will mean that they ignore the data given to them which in turn will lead to the model being underfit. In other words the model just won't pay attention to the information given to it and will insist that its priors are the best.

# 7H1

In 2007, *The Wall Street Journal* published an editorial with a graph of corporate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in, seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in `data(Laffer)`. Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue?

```{r}
#Load in and observe data
data(Laffer)
head(Laffer)
```

```{r}
#Make models
#Quickly checked Laffer to see what my priors should be. Not sure if I was supposed to do that
m7H1.1 <- quap(
  alist(
    tax_revenue ~ dnorm(mu, exp(sigma)),
    mu <- a + b*tax_rate,
    a ~ dnorm(20, 10),
    b ~ dnorm(0, 10),
    sigma ~ dnorm(0,1)
  ), data=Laffer
)

m7H1.2 <- quap(
  alist(
    tax_revenue ~ dnorm(mu, exp(sigma)),
    mu <- a + b[1]*tax_rate + b[2]*tax_rate,
    a ~ dnorm(20, 10),
    b ~ dnorm(0, 10),
    sigma ~ dnorm(0,1)
  ), data=Laffer, start = list(b=rep(0,2))
)
```

```{r}
#See what these curves should generally looks like
ggplot(data=Laffer)+
  aes(x=tax_rate, y=tax_revenue)+
  geom_point()+
  geom_smooth(method="lm", color="red", se=F)+
  stat_smooth(method="lm", formula = y ~ x + I(x^2), color="blue", se=F)
```

```{r}
#I can't read either of these outputs
compare(m7H1.1, m7H1.2, func = WAIC)
```

```{r}
compare(m7H1.1, m7H1.2, func = PSIS)
```

# 7H2

# 7H3

# 7H4

# 7H5
